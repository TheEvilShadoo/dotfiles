//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32139144
// Unknown Toolkit Version
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_75, texmode_independent
.address_size 64

	// .globl	DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope

.entry DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope(
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8,
	.param .u64 .ptr .global .align 8 DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9
)
{
	.reg .pred 	%p<263>;
	.reg .f32 	%f<13>;
	.reg .b32 	%r<328>;
	.reg .f64 	%fd<1501>;
	.reg .b64 	%rd<108>;


	ld.param.u64 	%rd48, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8];
	ld.param.u64 	%rd49, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9];
	mov.b32 	%r97, %envreg3;
	mov.u32 	%r98, %ctaid.x;
	mov.u32 	%r99, %ntid.x;
	mov.u32 	%r100, %tid.x;
	add.s32 	%r101, %r100, %r97;
	mad.lo.s32 	%r102, %r99, %r98, %r101;
	cvt.s64.s32 	%rd1, %r102;
	setp.gt.s32 	%p1, %r102, 5;
	mov.f64 	%fd1429, 0d0000000000000000;
	mov.f64 	%fd1430, %fd1429;
	mov.f64 	%fd1431, %fd1429;
	@%p1 bra 	$L__BB0_6;

	cvt.u32.u64 	%r104, %rd1;
	shl.b64 	%rd50, %rd1, 3;
	add.s64 	%rd97, %rd48, %rd50;
	add.s64 	%rd96, %rd49, %rd50;
	add.s32 	%r297, %r104, 1;
	mov.u32 	%r298, 1;

$L__BB0_2:
	ld.global.f64 	%fd4, [%rd97];
	ld.global.f64 	%fd5, [%rd96];
	abs.f64 	%fd216, %fd5;
	setp.gtu.f64 	%p2, %fd216, 0d7FF0000000000000;
	@%p2 bra 	$L__BB0_5;

	abs.f64 	%fd217, %fd4;
	setp.gtu.f64 	%p3, %fd217, 0d7FF0000000000000;
	@%p3 bra 	$L__BB0_5;

	add.f64 	%fd1431, %fd1431, %fd5;
	add.f64 	%fd1430, %fd1430, %fd4;
	add.f64 	%fd1429, %fd1429, 0d3FF0000000000000;

$L__BB0_5:
	setp.lt.u32 	%p4, %r298, 2;
	setp.lt.s32 	%p5, %r297, 6;
	and.pred  	%p6, %p4, %p5;
	add.s64 	%rd97, %rd97, 8;
	add.s64 	%rd96, %rd96, 8;
	add.s32 	%r298, %r298, 1;
	add.s32 	%r297, %r297, 1;
	@%p6 bra 	$L__BB0_2;

$L__BB0_6:
	setp.lt.f64 	%p7, %fd1429, 0d3FF0000000000000;
	mov.f64 	%fd1449, 0d7FF8000000000207;
	mov.f64 	%fd1438, 0d3FF0000000000000;
	@%p7 bra 	$L__BB0_65;

	setp.eq.f64 	%p8, %fd1429, 0d3FF0000000000000;
	@%p8 bra 	$L__BB0_33;

	abs.f64 	%fd15, %fd1429;
	setp.gtu.f64 	%p9, %fd15, 0d7FF0000000000000;
	@%p9 bra 	$L__BB0_32;
	bra.uni 	$L__BB0_9;

$L__BB0_32:
	add.f64 	%fd1438, %fd1429, 0dBFF0000000000000;
	bra.uni 	$L__BB0_33;

$L__BB0_9:
	setp.eq.f64 	%p10, %fd1429, 0d7FF0000000000000;
	@%p10 bra 	$L__BB0_31;
	bra.uni 	$L__BB0_10;

$L__BB0_31:
	mov.f64 	%fd412, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r137}, %fd412;
	}
	setp.gt.s32 	%p33, %r137, -1;
	selp.f64 	%fd1438, 0d7FF0000000000000, 0d0000000000000000, %p33;

$L__BB0_33:
	cvt.u32.u64 	%r138, %rd1;
	setp.gt.s32 	%p34, %r138, 5;
	mul.f64 	%fd34, %fd1431, %fd1438;
	mul.f64 	%fd35, %fd1430, %fd1438;
	mov.f64 	%fd1441, 0d0000000000000000;
	mov.f64 	%fd1442, %fd1441;
	@%p34 bra 	$L__BB0_39;

	ld.param.u64 	%rd87, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_9];
	ld.param.u64 	%rd86, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_8];
	shl.b64 	%rd53, %rd1, 3;
	add.s64 	%rd99, %rd86, %rd53;
	add.s64 	%rd98, %rd87, %rd53;
	add.s32 	%r303, %r138, 1;
	mov.u32 	%r304, 1;

$L__BB0_35:
	ld.global.f64 	%fd38, [%rd99];
	ld.global.f64 	%fd39, [%rd98];
	abs.f64 	%fd417, %fd39;
	setp.gtu.f64 	%p35, %fd417, 0d7FF0000000000000;
	@%p35 bra 	$L__BB0_38;

	abs.f64 	%fd418, %fd38;
	setp.gtu.f64 	%p36, %fd418, 0d7FF0000000000000;
	@%p36 bra 	$L__BB0_38;

	sub.f64 	%fd419, %fd39, %fd34;
	sub.f64 	%fd420, %fd38, %fd35;
	fma.rn.f64 	%fd1442, %fd419, %fd420, %fd1442;
	fma.rn.f64 	%fd1441, %fd419, %fd419, %fd1441;

$L__BB0_38:
	setp.lt.u32 	%p37, %r304, 2;
	setp.lt.s32 	%p38, %r303, 6;
	and.pred  	%p39, %p37, %p38;
	add.s64 	%rd99, %rd99, 8;
	add.s64 	%rd98, %rd98, 8;
	add.s32 	%r304, %r304, 1;
	add.s32 	%r303, %r303, 1;
	@%p39 bra 	$L__BB0_35;

$L__BB0_39:
	setp.eq.f64 	%p40, %fd1441, 0d0000000000000000;
	mov.f64 	%fd1449, 0d7FF8000000000214;
	mov.f64 	%fd1448, 0d3FF0000000000000;
	@%p40 bra 	$L__BB0_65;

	setp.eq.f64 	%p41, %fd1441, 0d3FF0000000000000;
	@%p41 bra 	$L__BB0_64;

	abs.f64 	%fd46, %fd1441;
	setp.gtu.f64 	%p42, %fd46, 0d7FF0000000000000;
	@%p42 bra 	$L__BB0_63;
	bra.uni 	$L__BB0_42;

$L__BB0_63:
	add.f64 	%fd1448, %fd1441, 0dBFF0000000000000;
	bra.uni 	$L__BB0_64;

$L__BB0_10:
	mov.f64 	%fd220, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r105, %temp}, %fd220;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r106}, %fd220;
	}
	and.b32  	%r107, %r106, 2147483647;
	setp.ne.s32 	%p11, %r107, 2146435072;
	setp.ne.s32 	%p12, %r105, 0;
	or.pred  	%p13, %p12, %p11;
	@%p13 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_11;

$L__BB0_13:
	mov.f64 	%fd225, 0d3FE0000000000000;
	mul.rn.f64 	%fd226, %fd225, %fd220;
	cvt.rzi.f64.f64 	%fd227, %fd226;
	mov.f64 	%fd228, 0d4000000000000000;
	mul.rn.f64 	%fd229, %fd228, %fd227;
	sub.f64 	%fd230, %fd220, %fd229;
	abs.f64 	%fd17, %fd230;
	setp.eq.f64 	%p16, %fd1429, 0d0000000000000000;
	@%p16 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_14;

$L__BB0_30:
	setp.eq.f64 	%p32, %fd17, 0d3FF0000000000000;
	rcp.rn.f64 	%fd409, %fd1429;
	mov.f64 	%fd410, 0d0000000000000000;
	rcp.rn.f64 	%fd411, %fd410;
	selp.f64 	%fd1438, %fd409, %fd411, %p32;
	bra.uni 	$L__BB0_33;

$L__BB0_42:
	setp.eq.f64 	%p43, %fd1441, 0d7FF0000000000000;
	@%p43 bra 	$L__BB0_62;
	bra.uni 	$L__BB0_43;

$L__BB0_62:
	mov.f64 	%fd612, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r173}, %fd612;
	}
	setp.gt.s32 	%p64, %r173, -1;
	selp.f64 	%fd1448, 0d7FF0000000000000, 0d0000000000000000, %p64;

$L__BB0_64:
	mul.f64 	%fd1449, %fd1442, %fd1448;

$L__BB0_65:
	cvt.u32.u64 	%r174, %rd1;
	setp.gt.s32 	%p65, %r174, 5;
	add.f64 	%fd66, %fd1449, 0d0000000000000000;
	mov.f64 	%fd1453, 0d0000000000000000;
	mov.f64 	%fd1454, %fd1453;
	mov.f64 	%fd1455, %fd1453;
	@%p65 bra 	$L__BB0_71;

	ld.param.u64 	%rd91, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6];
	ld.param.u64 	%rd90, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7];
	shl.b64 	%rd56, %rd1, 3;
	add.s64 	%rd101, %rd90, %rd56;
	add.s64 	%rd100, %rd91, %rd56;
	add.s32 	%r309, %r174, 1;
	mov.u32 	%r310, 1;

$L__BB0_67:
	ld.global.f64 	%fd70, [%rd101];
	ld.global.f64 	%fd71, [%rd100];
	abs.f64 	%fd619, %fd71;
	setp.gtu.f64 	%p66, %fd619, 0d7FF0000000000000;
	@%p66 bra 	$L__BB0_70;

	abs.f64 	%fd620, %fd70;
	setp.gtu.f64 	%p67, %fd620, 0d7FF0000000000000;
	@%p67 bra 	$L__BB0_70;

	add.f64 	%fd1454, %fd1454, %fd71;
	add.f64 	%fd1453, %fd1453, %fd70;
	add.f64 	%fd1455, %fd1455, 0d3FF0000000000000;

$L__BB0_70:
	setp.lt.u32 	%p68, %r310, 2;
	setp.lt.s32 	%p69, %r309, 6;
	and.pred  	%p70, %p68, %p69;
	add.s64 	%rd101, %rd101, 8;
	add.s64 	%rd100, %rd100, 8;
	add.s32 	%r310, %r310, 1;
	add.s32 	%r309, %r309, 1;
	@%p70 bra 	$L__BB0_67;

$L__BB0_71:
	setp.lt.f64 	%p71, %fd1455, 0d3FF0000000000000;
	mov.f64 	%fd1468, 0d7FF8000000000207;
	@%p71 bra 	$L__BB0_80;

	div.rn.f64 	%fd81, %fd1454, %fd1455;
	div.rn.f64 	%fd82, %fd1453, %fd1455;
	mov.f64 	%fd1462, 0d0000000000000000;
	mov.f64 	%fd1463, %fd1462;
	mov.f64 	%fd1464, %fd1462;
	@%p65 bra 	$L__BB0_78;

	ld.param.u64 	%rd89, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_6];
	ld.param.u64 	%rd88, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_7];
	shl.b64 	%rd57, %rd1, 3;
	add.s64 	%rd103, %rd88, %rd57;
	add.s64 	%rd102, %rd89, %rd57;
	add.s32 	%r311, %r174, 1;
	mov.u32 	%r312, 1;

$L__BB0_74:
	ld.global.f64 	%fd86, [%rd103];
	ld.global.f64 	%fd87, [%rd102];
	abs.f64 	%fd628, %fd87;
	setp.gtu.f64 	%p73, %fd628, 0d7FF0000000000000;
	@%p73 bra 	$L__BB0_77;

	abs.f64 	%fd629, %fd86;
	setp.gtu.f64 	%p74, %fd629, 0d7FF0000000000000;
	@%p74 bra 	$L__BB0_77;

	sub.f64 	%fd630, %fd87, %fd81;
	sub.f64 	%fd631, %fd86, %fd82;
	fma.rn.f64 	%fd1462, %fd630, %fd631, %fd1462;
	fma.rn.f64 	%fd1464, %fd630, %fd630, %fd1464;
	fma.rn.f64 	%fd1463, %fd631, %fd631, %fd1463;

$L__BB0_77:
	setp.lt.u32 	%p75, %r312, 2;
	setp.lt.s32 	%p76, %r311, 6;
	and.pred  	%p77, %p75, %p76;
	add.s64 	%rd103, %rd103, 8;
	add.s64 	%rd102, %rd102, 8;
	add.s32 	%r312, %r312, 1;
	add.s32 	%r311, %r311, 1;
	@%p77 bra 	$L__BB0_74;

$L__BB0_78:
	setp.eq.f64 	%p78, %fd1463, 0d0000000000000000;
	setp.eq.f64 	%p79, %fd1464, 0d0000000000000000;
	or.pred  	%p80, %p78, %p79;
	mov.f64 	%fd1468, 0d7FF8000000000214;
	@%p80 bra 	$L__BB0_80;

	mul.f64 	%fd633, %fd1463, %fd1464;
	sqrt.rn.f64 	%fd634, %fd633;
	div.rn.f64 	%fd1468, %fd1462, %fd634;

$L__BB0_80:
	add.f64 	%fd99, %fd1468, 0d0000000000000000;
	mov.f64 	%fd1473, 0d0000000000000000;
	mov.u32 	%r317, 0;
	mov.f64 	%fd1474, %fd1473;
	@%p65 bra 	$L__BB0_86;

	ld.param.u64 	%rd95, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4];
	ld.param.u64 	%rd93, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5];
	shl.b64 	%rd58, %rd1, 3;
	add.s64 	%rd105, %rd93, %rd58;
	add.s64 	%rd104, %rd95, %rd58;
	add.s32 	%r313, %r174, 1;
	mov.u32 	%r314, 1;

$L__BB0_82:
	ld.global.f64 	%fd1472, [%rd104];
	abs.f64 	%fd639, %fd1472;
	setp.gtu.f64 	%p82, %fd639, 0d7FF0000000000000;
	@%p82 bra 	$L__BB0_84;

	ld.global.f64 	%fd1471, [%rd105];
	abs.f64 	%fd640, %fd1471;
	setp.le.f64 	%p83, %fd640, 0d7FF0000000000000;
	@%p83 bra 	$L__BB0_85;

$L__BB0_84:
	add.s32 	%r317, %r317, -1;
	mov.f64 	%fd1471, 0d0000000000000000;
	mov.f64 	%fd1472, %fd1471;

$L__BB0_85:
	add.s32 	%r317, %r317, 1;
	add.f64 	%fd1474, %fd1474, %fd1472;
	add.f64 	%fd1473, %fd1473, %fd1471;
	setp.lt.s32 	%p84, %r313, 6;
	setp.lt.u32 	%p85, %r314, 2;
	and.pred  	%p86, %p85, %p84;
	add.s64 	%rd105, %rd105, 8;
	add.s64 	%rd104, %rd104, 8;
	add.s32 	%r314, %r314, 1;
	add.s32 	%r313, %r313, 1;
	@%p86 bra 	$L__BB0_82;

$L__BB0_86:
	setp.lt.s32 	%p87, %r317, 1;
	mov.f64 	%fd1479, 0d7FF8000000000207;
	@%p87 bra 	$L__BB0_94;

	cvt.rn.f64.s32 	%fd110, %r317;
	mov.f64 	%fd1478, 0d0000000000000000;
	@%p65 bra 	$L__BB0_93;

	ld.param.u64 	%rd94, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_4];
	ld.param.u64 	%rd92, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_5];
	div.rn.f64 	%fd111, %fd1474, %fd110;
	div.rn.f64 	%fd112, %fd1473, %fd110;
	shl.b64 	%rd59, %rd1, 3;
	add.s64 	%rd107, %rd92, %rd59;
	add.s64 	%rd106, %rd94, %rd59;
	add.s32 	%r318, %r174, 1;
	mov.u32 	%r319, 1;

$L__BB0_89:
	ld.global.f64 	%fd114, [%rd106];
	abs.f64 	%fd646, %fd114;
	setp.gtu.f64 	%p89, %fd646, 0d7FF0000000000000;
	mov.f64 	%fd1476, %fd111;
	mov.f64 	%fd1477, %fd112;
	@%p89 bra 	$L__BB0_92;

	ld.global.f64 	%fd115, [%rd107];
	abs.f64 	%fd647, %fd115;
	setp.gtu.f64 	%p90, %fd647, 0d7FF0000000000000;
	mov.f64 	%fd1476, %fd111;
	mov.f64 	%fd1477, %fd112;
	@%p90 bra 	$L__BB0_92;

	mov.f64 	%fd1476, %fd114;
	mov.f64 	%fd1477, %fd115;

$L__BB0_92:
	sub.f64 	%fd648, %fd1477, %fd112;
	sub.f64 	%fd649, %fd1476, %fd111;
	fma.rn.f64 	%fd1478, %fd649, %fd648, %fd1478;
	setp.lt.s32 	%p91, %r318, 6;
	setp.lt.u32 	%p92, %r319, 2;
	and.pred  	%p93, %p92, %p91;
	add.s64 	%rd107, %rd107, 8;
	add.s64 	%rd106, %rd106, 8;
	add.s32 	%r319, %r319, 1;
	add.s32 	%r318, %r318, 1;
	@%p93 bra 	$L__BB0_89;

$L__BB0_93:
	div.rn.f64 	%fd1479, %fd1478, %fd110;

$L__BB0_94:
	setp.gt.s32 	%p94, %r174, 4;
	mov.f64 	%fd1480, 0d8000000000000000;
	@%p94 bra 	$L__BB0_97;

	ld.param.u64 	%rd85, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_3];
	shl.b64 	%rd60, %rd1, 3;
	add.s64 	%rd61, %rd85, %rd60;
	ld.global.f64 	%fd122, [%rd61];
	abs.f64 	%fd652, %fd122;
	setp.gtu.f64 	%p95, %fd652, 0d7FF0000000000000;
	@%p95 bra 	$L__BB0_97;

	mul.f64 	%fd1480, %fd122, 0dBFE6A09E667F3BCC;

$L__BB0_97:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r189}, %fd1480; 
	}
	// end inline asm
	setp.lt.s32 	%p96, %r189, 1072168960;
	@%p96 bra 	$L__BB0_102;
	bra.uni 	$L__BB0_98;

$L__BB0_102:
	abs.f64 	%fd900, %fd1480;
	mov.f64 	%fd901, 0d3D47088FDB46FA5F;
	mov.f64 	%fd902, 0dBCF0679AFBA6F279;
	fma.rn.f64 	%fd903, %fd902, %fd900, %fd901;
	mov.f64 	%fd904, 0dBD8DF9F9B976A9B2;
	fma.rn.f64 	%fd905, %fd903, %fd900, %fd904;
	mov.f64 	%fd906, 0d3DC7F1F5590CC332;
	fma.rn.f64 	%fd907, %fd905, %fd900, %fd906;
	mov.f64 	%fd908, 0dBDFA28A3CD2D56C4;
	fma.rn.f64 	%fd909, %fd907, %fd900, %fd908;
	mov.f64 	%fd910, 0d3E2485EE67835925;
	fma.rn.f64 	%fd911, %fd909, %fd900, %fd910;
	mov.f64 	%fd912, 0dBE476DB45919F583;
	fma.rn.f64 	%fd913, %fd911, %fd900, %fd912;
	mov.f64 	%fd914, 0d3E62D698D98C8D71;
	fma.rn.f64 	%fd915, %fd913, %fd900, %fd914;
	mov.f64 	%fd916, 0dBE720A2C7155D5C6;
	fma.rn.f64 	%fd917, %fd915, %fd900, %fd916;
	mov.f64 	%fd918, 0dBE41D29B37CA1397;
	fma.rn.f64 	%fd919, %fd917, %fd900, %fd918;
	mov.f64 	%fd920, 0d3EA2EF6CC0F67A49;
	fma.rn.f64 	%fd921, %fd919, %fd900, %fd920;
	mov.f64 	%fd922, 0dBEC102B892333B6F;
	fma.rn.f64 	%fd923, %fd921, %fd900, %fd922;
	mov.f64 	%fd924, 0d3ECA30375BA9A84E;
	fma.rn.f64 	%fd925, %fd923, %fd900, %fd924;
	mov.f64 	%fd926, 0d3ECAAD18DEDEA43E;
	fma.rn.f64 	%fd927, %fd925, %fd900, %fd926;
	mov.f64 	%fd928, 0dBEFF05355BC5B225;
	fma.rn.f64 	%fd929, %fd927, %fd900, %fd928;
	mov.f64 	%fd930, 0d3F10E37A3108BC8B;
	fma.rn.f64 	%fd931, %fd929, %fd900, %fd930;
	mov.f64 	%fd932, 0d3EFB292D828E5CB2;
	fma.rn.f64 	%fd933, %fd931, %fd900, %fd932;
	mov.f64 	%fd934, 0dBF4356626EBF9BFA;
	fma.rn.f64 	%fd935, %fd933, %fd900, %fd934;
	mov.f64 	%fd936, 0d3F5BCA68F73D6AFC;
	fma.rn.f64 	%fd937, %fd935, %fd900, %fd936;
	mov.f64 	%fd938, 0dBF2B6B69EBBC280B;
	fma.rn.f64 	%fd939, %fd937, %fd900, %fd938;
	mov.f64 	%fd940, 0dBF9396685912A453;
	fma.rn.f64 	%fd941, %fd939, %fd900, %fd940;
	mov.f64 	%fd942, 0d3FBA4F4E2A1ABEF8;
	fma.rn.f64 	%fd943, %fd941, %fd900, %fd942;
	mov.f64 	%fd944, 0d3FE45F306DC9C8BB;
	fma.rn.f64 	%fd945, %fd943, %fd900, %fd944;
	mov.f64 	%fd946, 0d3FC06EBA8214DB69;
	fma.rn.f64 	%fd947, %fd945, %fd900, %fd946;
	fma.rn.f64 	%fd948, %fd947, %fd900, %fd900;
	sub.f64 	%fd949, %fd900, %fd948;
	fma.rn.f64 	%fd950, %fd947, %fd900, %fd949;
	neg.f64 	%fd951, %fd948;
	neg.f64 	%fd952, %fd950;
	cvt.rn.f32.f64 	%f7, %fd948;
	mul.f32 	%f8, %f7, 0fBFB8AA3B;
	cvt.rni.f32.f32 	%f9, %f8;
	cvt.f64.f32 	%fd953, %f9;
	neg.f64 	%fd954, %fd953;
	mov.f64 	%fd955, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd956, %fd954, %fd955, %fd951;
	mov.f64 	%fd957, 0d3E928A27F89B6999;
	mov.f64 	%fd958, 0d3E5AE904A4741B81;
	fma.rn.f64 	%fd959, %fd958, %fd956, %fd957;
	mov.f64 	%fd960, 0d3EC71DE715FF7E07;
	fma.rn.f64 	%fd961, %fd959, %fd956, %fd960;
	mov.f64 	%fd962, 0d3EFA019A6B0AC45A;
	fma.rn.f64 	%fd963, %fd961, %fd956, %fd962;
	mov.f64 	%fd964, 0d3F2A01A017EED94F;
	fma.rn.f64 	%fd965, %fd963, %fd956, %fd964;
	mov.f64 	%fd966, 0d3F56C16C17F2A71B;
	fma.rn.f64 	%fd967, %fd965, %fd956, %fd966;
	mov.f64 	%fd968, 0d3F811111111173C4;
	fma.rn.f64 	%fd969, %fd967, %fd956, %fd968;
	mov.f64 	%fd970, 0d3FA555555555211A;
	fma.rn.f64 	%fd971, %fd969, %fd956, %fd970;
	mov.f64 	%fd972, 0d3FC5555555555540;
	fma.rn.f64 	%fd973, %fd971, %fd956, %fd972;
	mov.f64 	%fd974, 0d3FE0000000000005;
	fma.rn.f64 	%fd975, %fd973, %fd956, %fd974;
	mul.f64 	%fd976, %fd956, %fd975;
	fma.rn.f64 	%fd977, %fd976, %fd956, %fd952;
	add.f64 	%fd978, %fd956, %fd977;
	ex2.approx.ftz.f32 	%f10, %f9;
	cvt.f64.f32 	%fd979, %f10;
	mov.f64 	%fd980, 0d3FF0000000000000;
	sub.f64 	%fd981, %fd980, %fd979;
	neg.f64 	%fd982, %fd978;
	fma.rn.f64 	%fd983, %fd982, %fd979, %fd981;
	setp.ge.f64 	%p101, %fd900, 0d4017AFB48DC96626;
	selp.f64 	%fd984, 0d3FF0000000000000, %fd983, %p101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r212, %temp}, %fd984;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r213}, %fd984;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r214}, %fd1480;
	}
	and.b32  	%r215, %r214, -2147483648;
	or.b32  	%r216, %r213, %r215;
	mov.b64 	%fd985, {%r212, %r216};
	sub.f64 	%fd1481, %fd980, %fd985;
	bra.uni 	$L__BB0_103;

$L__BB0_98:
	setp.gt.f64 	%p97, %fd1480, 0d403B4CCCCCCCCCCD;
	mov.f64 	%fd1481, 0d0000000000000000;
	@%p97 bra 	$L__BB0_103;

	setp.lt.s32 	%p98, %r189, 1075052544;
	@%p98 bra 	$L__BB0_101;
	bra.uni 	$L__BB0_100;

$L__BB0_101:
	mov.f64 	%fd773, 0d3FE20DD7452FBC22;
	mov.f64 	%fd775, 0d401FD453E105E9A2;
	// begin inline asm
	fma.rn.f64 	%fd772, %fd773, %fd1480, %fd775;
	// end inline asm
	mov.f64 	%fd779, 0d404B26245B951FB4;
	// begin inline asm
	fma.rn.f64 	%fd776, %fd772, %fd1480, %fd779;
	// end inline asm
	mov.f64 	%fd783, 0d406C7835DC0F1F49;
	// begin inline asm
	fma.rn.f64 	%fd780, %fd776, %fd1480, %fd783;
	// end inline asm
	mov.f64 	%fd787, 0d4083AFA471E5C766;
	// begin inline asm
	fma.rn.f64 	%fd784, %fd780, %fd1480, %fd787;
	// end inline asm
	mov.f64 	%fd791, 0d4091FB514824F49F;
	// begin inline asm
	fma.rn.f64 	%fd788, %fd784, %fd1480, %fd791;
	// end inline asm
	mov.f64 	%fd795, 0d409450DDEE8272BB;
	// begin inline asm
	fma.rn.f64 	%fd792, %fd788, %fd1480, %fd795;
	// end inline asm
	mov.f64 	%fd799, 0d4086B952E4ECBC50;
	// begin inline asm
	fma.rn.f64 	%fd796, %fd792, %fd1480, %fd799;
	// end inline asm
	add.f64 	%fd801, %fd1480, 0d402C35442E99E667;
	mov.f64 	%fd803, 0d40582F68071A079D;
	// begin inline asm
	fma.rn.f64 	%fd800, %fd801, %fd1480, %fd803;
	// end inline asm
	mov.f64 	%fd807, 0d4079ABD39A029DAA;
	// begin inline asm
	fma.rn.f64 	%fd804, %fd800, %fd1480, %fd807;
	// end inline asm
	mov.f64 	%fd811, 0d409230CA327093FD;
	// begin inline asm
	fma.rn.f64 	%fd808, %fd804, %fd1480, %fd811;
	// end inline asm
	mov.f64 	%fd815, 0d40A174FAB33B54A7;
	// begin inline asm
	fma.rn.f64 	%fd812, %fd808, %fd1480, %fd815;
	// end inline asm
	mov.f64 	%fd819, 0d40A601508230F980;
	// begin inline asm
	fma.rn.f64 	%fd816, %fd812, %fd1480, %fd819;
	// end inline asm
	mov.f64 	%fd823, 0d40A091785EC9331E;
	// begin inline asm
	fma.rn.f64 	%fd820, %fd816, %fd1480, %fd823;
	// end inline asm
	mov.f64 	%fd827, 0d4086B952E52F3622;
	// begin inline asm
	fma.rn.f64 	%fd824, %fd820, %fd1480, %fd827;
	// end inline asm
	div.rn.f64 	%fd893, %fd796, %fd824;
	mul.rn.f64 	%fd894, %fd1480, %fd1480;
	neg.f64 	%fd835, %fd894;
	// begin inline asm
	fma.rn.f64 	%fd828, %fd1480, %fd1480, %fd835;
	// end inline asm
	mov.f64 	%fd895, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd896, %fd835, %fd895;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r203}, %fd896;
	}
	and.b32  	%r204, %r203, -2147483648;
	mov.f64 	%fd879, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r205}, %fd879;
	}
	or.b32  	%r206, %r205, %r204;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r207, %temp}, %fd879;
	}
	mov.b64 	%fd897, {%r207, %r206};
	add.rz.f64 	%fd898, %fd896, %fd897;
	cvt.rzi.f64.f64 	%fd837, %fd898;
	cvt.rzi.s32.f64 	%r208, %fd837;
	mov.f64 	%fd834, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd832, %fd837, %fd834, %fd835;
	// end inline asm
	mov.f64 	%fd838, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd836, %fd837, %fd838, %fd832;
	// end inline asm
	setp.lt.s32 	%p100, %r208, -1020;
	selp.f64 	%fd899, 0d3C90000000000000, 0d4000000000000000, %p100;
	mov.f64 	%fd841, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd843, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd840, %fd841, %fd836, %fd843;
	// end inline asm
	mov.f64 	%fd847, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd844, %fd840, %fd836, %fd847;
	// end inline asm
	mov.f64 	%fd851, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd848, %fd844, %fd836, %fd851;
	// end inline asm
	mov.f64 	%fd855, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd852, %fd848, %fd836, %fd855;
	// end inline asm
	mov.f64 	%fd859, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd856, %fd852, %fd836, %fd859;
	// end inline asm
	mov.f64 	%fd863, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd860, %fd856, %fd836, %fd863;
	// end inline asm
	mov.f64 	%fd867, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd864, %fd860, %fd836, %fd867;
	// end inline asm
	mov.f64 	%fd871, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd868, %fd864, %fd836, %fd871;
	// end inline asm
	mov.f64 	%fd875, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd872, %fd868, %fd836, %fd875;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd876, %fd872, %fd836, %fd879;
	// end inline asm
	mul.rn.f64 	%fd881, %fd876, %fd836;
	// begin inline asm
	fma.rn.f64 	%fd880, %fd881, %fd836, %fd836;
	// end inline asm
	shl.b32 	%r209, %r208, 20;
	add.s32 	%r210, %r209, 57671680;
	selp.b32 	%r211, %r210, %r209, %p100;
	add.s32 	%r202, %r211, 1071644672;
	mov.u32 	%r201, 0;
	// begin inline asm
	mov.b64 	%fd884, {%r201, %r202};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd885, %fd880, %fd884, %fd884;
	// end inline asm
	mul.rn.f64 	%fd892, %fd885, %fd899;
	neg.f64 	%fd891, %fd892;
	// begin inline asm
	fma.rn.f64 	%fd889, %fd828, %fd891, %fd892;
	// end inline asm
	mul.rn.f64 	%fd1481, %fd893, %fd889;
	bra.uni 	$L__BB0_103;

$L__BB0_100:
	rcp.rn.f64 	%fd764, %fd1480;
	mul.rn.f64 	%fd697, %fd764, %fd764;
	mov.f64 	%fd656, 0dC1186DF84479631D;
	mov.f64 	%fd658, 0d41019A6E9A7FFBB8;
	// begin inline asm
	fma.rn.f64 	%fd655, %fd656, %fd697, %fd658;
	// end inline asm
	mov.f64 	%fd662, 0dC0DB040BE3D5CA18;
	// begin inline asm
	fma.rn.f64 	%fd659, %fd655, %fd697, %fd662;
	// end inline asm
	mov.f64 	%fd666, 0d40B012760EE009A0;
	// begin inline asm
	fma.rn.f64 	%fd663, %fd659, %fd697, %fd666;
	// end inline asm
	mov.f64 	%fd670, 0dC082587AE4008D0E;
	// begin inline asm
	fma.rn.f64 	%fd667, %fd663, %fd697, %fd670;
	// end inline asm
	mov.f64 	%fd674, 0d4056DF5D938ACAFE;
	// begin inline asm
	fma.rn.f64 	%fd671, %fd667, %fd697, %fd674;
	// end inline asm
	mov.f64 	%fd678, 0dC030A8D46D765681;
	// begin inline asm
	fma.rn.f64 	%fd675, %fd671, %fd697, %fd678;
	// end inline asm
	mov.f64 	%fd682, 0d400D9EAE0C665C75;
	// begin inline asm
	fma.rn.f64 	%fd679, %fd675, %fd697, %fd682;
	// end inline asm
	mov.f64 	%fd686, 0dBFF0ECF9C8880942;
	// begin inline asm
	fma.rn.f64 	%fd683, %fd679, %fd697, %fd686;
	// end inline asm
	mov.f64 	%fd690, 0d3FDB14C2F82A33F7;
	// begin inline asm
	fma.rn.f64 	%fd687, %fd683, %fd697, %fd690;
	// end inline asm
	mov.f64 	%fd694, 0dBFD20DD75042844F;
	// begin inline asm
	fma.rn.f64 	%fd691, %fd687, %fd697, %fd694;
	// end inline asm
	mov.f64 	%fd698, 0d3FE20DD750429B6B;
	// begin inline asm
	fma.rn.f64 	%fd695, %fd691, %fd697, %fd698;
	// end inline asm
	mul.rn.f64 	%fd765, %fd1480, %fd1480;
	neg.f64 	%fd706, %fd765;
	// begin inline asm
	fma.rn.f64 	%fd699, %fd1480, %fd1480, %fd706;
	// end inline asm
	mov.f64 	%fd766, 0d3FF71547652B82FE;
	mul.rn.f64 	%fd767, %fd706, %fd766;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r192}, %fd767;
	}
	and.b32  	%r193, %r192, -2147483648;
	mov.f64 	%fd750, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r194}, %fd750;
	}
	or.b32  	%r195, %r194, %r193;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r196, %temp}, %fd750;
	}
	mov.b64 	%fd768, {%r196, %r195};
	add.rz.f64 	%fd769, %fd767, %fd768;
	cvt.rzi.f64.f64 	%fd708, %fd769;
	cvt.rzi.s32.f64 	%r197, %fd708;
	mov.f64 	%fd705, 0dBFE62E42FEFA39EF;
	// begin inline asm
	fma.rn.f64 	%fd703, %fd708, %fd705, %fd706;
	// end inline asm
	mov.f64 	%fd709, 0dBC7ABC9E3B39803F;
	// begin inline asm
	fma.rn.f64 	%fd707, %fd708, %fd709, %fd703;
	// end inline asm
	setp.lt.s32 	%p99, %r197, -1020;
	selp.f64 	%fd770, 0d3C90000000000000, 0d4000000000000000, %p99;
	mov.f64 	%fd712, 0d3E21F07FCCF58BAD;
	mov.f64 	%fd714, 0d3E5AFD81DA6C3BAF;
	// begin inline asm
	fma.rn.f64 	%fd711, %fd712, %fd707, %fd714;
	// end inline asm
	mov.f64 	%fd718, 0d3E927E55F60F80E6;
	// begin inline asm
	fma.rn.f64 	%fd715, %fd711, %fd707, %fd718;
	// end inline asm
	mov.f64 	%fd722, 0d3EC71DDA8F02D666;
	// begin inline asm
	fma.rn.f64 	%fd719, %fd715, %fd707, %fd722;
	// end inline asm
	mov.f64 	%fd726, 0d3EFA01A013B894E0;
	// begin inline asm
	fma.rn.f64 	%fd723, %fd719, %fd707, %fd726;
	// end inline asm
	mov.f64 	%fd730, 0d3F2A01A01D3AF788;
	// begin inline asm
	fma.rn.f64 	%fd727, %fd723, %fd707, %fd730;
	// end inline asm
	mov.f64 	%fd734, 0d3F56C16C16C3A1EC;
	// begin inline asm
	fma.rn.f64 	%fd731, %fd727, %fd707, %fd734;
	// end inline asm
	mov.f64 	%fd738, 0d3F81111111109161;
	// begin inline asm
	fma.rn.f64 	%fd735, %fd731, %fd707, %fd738;
	// end inline asm
	mov.f64 	%fd742, 0d3FA55555555554C1;
	// begin inline asm
	fma.rn.f64 	%fd739, %fd735, %fd707, %fd742;
	// end inline asm
	mov.f64 	%fd746, 0d3FC555555555556F;
	// begin inline asm
	fma.rn.f64 	%fd743, %fd739, %fd707, %fd746;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd747, %fd743, %fd707, %fd750;
	// end inline asm
	mul.rn.f64 	%fd752, %fd747, %fd707;
	// begin inline asm
	fma.rn.f64 	%fd751, %fd752, %fd707, %fd707;
	// end inline asm
	shl.b32 	%r198, %r197, 20;
	add.s32 	%r199, %r198, 57671680;
	selp.b32 	%r200, %r199, %r198, %p99;
	add.s32 	%r191, %r200, 1071644672;
	mov.u32 	%r190, 0;
	// begin inline asm
	mov.b64 	%fd755, {%r190, %r191};
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd756, %fd751, %fd755, %fd755;
	// end inline asm
	mul.rn.f64 	%fd763, %fd756, %fd770;
	neg.f64 	%fd762, %fd763;
	// begin inline asm
	fma.rn.f64 	%fd760, %fd699, %fd762, %fd763;
	// end inline asm
	mul.rn.f64 	%fd771, %fd695, %fd764;
	mul.rn.f64 	%fd1481, %fd771, %fd760;

$L__BB0_103:
	ld.param.u64 	%rd82, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_2];
	fma.rn.f64 	%fd129, %fd1481, 0d3FE0000000000000, 0d0000000000000000;
	mov.f64 	%fd1484, 0d0000000000000000;
	shl.b64 	%rd62, %rd1, 3;
	add.s64 	%rd38, %rd82, %rd62;
	mov.f64 	%fd1485, %fd1484;
	@%p94 bra 	$L__BB0_105;

	ld.global.f64 	%fd988, [%rd38];
	abs.f64 	%fd989, %fd988;
	setp.le.f64 	%p103, %fd989, 0d7FF0000000000000;
	add.f64 	%fd990, %fd988, 0d0000000000000000;
	selp.f64 	%fd1484, 0d3FF0000000000000, 0d0000000000000000, %p103;
	selp.f64 	%fd1485, %fd990, 0d0000000000000000, %p103;

$L__BB0_105:
	ld.param.u64 	%rd83, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_1];
	add.s64 	%rd39, %rd83, %rd62;
	@%p94 bra 	$L__BB0_108;

	ld.global.f64 	%fd134, [%rd39];
	abs.f64 	%fd991, %fd134;
	setp.gtu.f64 	%p105, %fd991, 0d7FF0000000000000;
	@%p105 bra 	$L__BB0_108;

	add.f64 	%fd1485, %fd1485, %fd134;
	add.f64 	%fd1484, %fd1484, 0d3FF0000000000000;

$L__BB0_108:
	setp.eq.f64 	%p106, %fd1484, 0d3FF0000000000000;
	mov.f64 	%fd1489, 0d3FF0000000000000;
	@%p106 bra 	$L__BB0_134;

	abs.f64 	%fd139, %fd1484;
	setp.gtu.f64 	%p107, %fd139, 0d7FF0000000000000;
	@%p107 bra 	$L__BB0_133;
	bra.uni 	$L__BB0_110;

$L__BB0_133:
	add.f64 	%fd1489, %fd1484, 0dBFF0000000000000;
	bra.uni 	$L__BB0_134;

$L__BB0_110:
	setp.eq.f64 	%p108, %fd1484, 0d7FF0000000000000;
	@%p108 bra 	$L__BB0_132;
	bra.uni 	$L__BB0_111;

$L__BB0_132:
	mov.f64 	%fd1185, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r251}, %fd1185;
	}
	setp.gt.s32 	%p131, %r251, -1;
	selp.f64 	%fd1489, 0d7FF0000000000000, 0d0000000000000000, %p131;

$L__BB0_134:
	mul.f64 	%fd158, %fd1485, %fd1489;
	mov.f64 	%fd1491, 0d0000000000000000;
	@%p94 bra 	$L__BB0_137;

	ld.global.f64 	%fd159, [%rd38];
	abs.f64 	%fd1188, %fd159;
	setp.gtu.f64 	%p133, %fd1188, 0d7FF0000000000000;
	@%p133 bra 	$L__BB0_137;

	sub.f64 	%fd1189, %fd159, %fd158;
	fma.rn.f64 	%fd1491, %fd1189, %fd1189, 0d0000000000000000;

$L__BB0_137:
	@%p94 bra 	$L__BB0_140;

	ld.global.f64 	%fd162, [%rd39];
	abs.f64 	%fd1190, %fd162;
	setp.gtu.f64 	%p135, %fd1190, 0d7FF0000000000000;
	@%p135 bra 	$L__BB0_140;

	sub.f64 	%fd1191, %fd162, %fd158;
	fma.rn.f64 	%fd1491, %fd1191, %fd1191, %fd1491;

$L__BB0_140:
	setp.le.f64 	%p136, %fd1484, 0d3FF0000000000000;
	mov.f64 	%fd1496, 0d7FF8000000000214;
	mov.f64 	%fd1495, 0d3FF0000000000000;
	@%p136 bra 	$L__BB0_168;

	add.f64 	%fd165, %fd1484, 0dBFF0000000000000;
	setp.eq.f64 	%p137, %fd165, 0d3FF0000000000000;
	@%p137 bra 	$L__BB0_167;

	abs.f64 	%fd166, %fd165;
	setp.gtu.f64 	%p138, %fd166, 0d7FF0000000000000;
	@%p138 bra 	$L__BB0_166;
	bra.uni 	$L__BB0_143;

$L__BB0_166:
	add.f64 	%fd1495, %fd165, 0dBFF0000000000000;
	bra.uni 	$L__BB0_167;

$L__BB0_143:
	setp.eq.f64 	%p139, %fd165, 0d7FF0000000000000;
	@%p139 bra 	$L__BB0_165;
	bra.uni 	$L__BB0_144;

$L__BB0_165:
	mov.f64 	%fd1386, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r286}, %fd1386;
	}
	setp.gt.s32 	%p162, %r286, -1;
	selp.f64 	%fd1495, 0d7FF0000000000000, 0d0000000000000000, %p162;

$L__BB0_167:
	mul.f64 	%fd1496, %fd1491, %fd1495;

$L__BB0_168:
	setp.gt.f64 	%p163, %fd129, 0d0000000000000000;
	setp.lt.f64 	%p164, %fd1496, 0d0000000000000000;
	and.pred  	%p165, %p163, %p164;
	@%p165 bra 	$L__BB0_170;

	setp.geu.f64 	%p166, %fd129, 0d0000000000000000;
	setp.leu.f64 	%p167, %fd1496, 0d0000000000000000;
	or.pred  	%p168, %p166, %p167;
	@%p168 bra 	$L__BB0_179;

$L__BB0_170:
	neg.f64 	%fd187, %fd129;
	setp.eq.f64 	%p169, %fd1496, %fd187;
	mov.f64 	%fd1497, 0d0000000000000000;
	@%p169 bra 	$L__BB0_180;

	setp.eq.f64 	%p170, %fd1496, 0d0000000000000000;
	setp.eq.f64 	%p171, %fd129, 0d8000000000000000;
	or.pred  	%p172, %p171, %p170;
	@%p172 bra 	$L__BB0_179;

	add.f64 	%fd1388, %fd129, %fd1496;
	abs.f64 	%fd188, %fd1388;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r287}, %fd188;
	}
	and.b32  	%r288, %r287, 2146435072;
	setp.eq.s32 	%p173, %r288, 2146435072;
	@%p173 bra 	$L__BB0_179;

	abs.f64 	%fd189, %fd1496;
	mul.f64 	%fd1389, %fd189, 0d3D30000000000000;
	setp.gt.f64 	%p174, %fd188, %fd1389;
	@%p174 bra 	$L__BB0_179;

	abs.f64 	%fd190, %fd187;
	mul.f64 	%fd1390, %fd190, 0d3D30000000000000;
	setp.gt.f64 	%p175, %fd188, %fd1390;
	@%p175 bra 	$L__BB0_179;

	setp.gtu.f64 	%p176, %fd188, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd68, %fd188;
	setp.gt.s64 	%p177, %rd68, 9007199254740991;
	or.pred  	%p178, %p176, %p177;
	@%p178 bra 	$L__BB0_178;

	setp.gtu.f64 	%p179, %fd189, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd69, %fd189;
	setp.gt.s64 	%p180, %rd69, 9007199254740991;
	or.pred  	%p181, %p179, %p180;
	@%p181 bra 	$L__BB0_178;

	setp.le.f64 	%p182, %fd190, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd70, %fd190;
	setp.lt.s64 	%p183, %rd70, 9007199254740992;
	and.pred  	%p184, %p182, %p183;
	@%p184 bra 	$L__BB0_179;

$L__BB0_178:
	mul.f64 	%fd1392, %fd189, 0d3CF0000000000000;
	setp.lt.f64 	%p185, %fd188, %fd1392;
	mul.f64 	%fd1393, %fd190, 0d3CF0000000000000;
	setp.lt.f64 	%p186, %fd188, %fd1393;
	and.pred  	%p187, %p185, %p186;
	@%p187 bra 	$L__BB0_180;

$L__BB0_179:
	add.f64 	%fd1497, %fd129, %fd1496;

$L__BB0_180:
	setp.lt.f64 	%p188, %fd1479, 0d0000000000000000;
	setp.lt.f64 	%p189, %fd1497, 0d0000000000000000;
	and.pred  	%p190, %p188, %p189;
	@%p190 bra 	$L__BB0_182;

	setp.leu.f64 	%p191, %fd1497, 0d0000000000000000;
	setp.leu.f64 	%p192, %fd1479, 0d0000000000000000;
	or.pred  	%p193, %p192, %p191;
	@%p193 bra 	$L__BB0_191;

$L__BB0_182:
	setp.eq.f64 	%p194, %fd1497, %fd1479;
	mov.f64 	%fd1498, 0d0000000000000000;
	@%p194 bra 	$L__BB0_192;

	setp.eq.f64 	%p195, %fd1497, 0d0000000000000000;
	setp.eq.f64 	%p196, %fd1479, 0d0000000000000000;
	or.pred  	%p197, %p196, %p195;
	@%p197 bra 	$L__BB0_191;

	sub.f64 	%fd1395, %fd1497, %fd1479;
	abs.f64 	%fd193, %fd1395;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r289}, %fd193;
	}
	and.b32  	%r290, %r289, 2146435072;
	setp.eq.s32 	%p198, %r290, 2146435072;
	@%p198 bra 	$L__BB0_191;

	abs.f64 	%fd194, %fd1497;
	mul.f64 	%fd1396, %fd194, 0d3D30000000000000;
	setp.gt.f64 	%p199, %fd193, %fd1396;
	@%p199 bra 	$L__BB0_191;

	abs.f64 	%fd195, %fd1479;
	mul.f64 	%fd1397, %fd195, 0d3D30000000000000;
	setp.gt.f64 	%p200, %fd193, %fd1397;
	@%p200 bra 	$L__BB0_191;

	setp.gtu.f64 	%p201, %fd193, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd71, %fd193;
	setp.gt.s64 	%p202, %rd71, 9007199254740991;
	or.pred  	%p203, %p201, %p202;
	@%p203 bra 	$L__BB0_190;

	setp.gtu.f64 	%p204, %fd194, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd72, %fd194;
	setp.gt.s64 	%p205, %rd72, 9007199254740991;
	or.pred  	%p206, %p204, %p205;
	@%p206 bra 	$L__BB0_190;

	setp.le.f64 	%p207, %fd195, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd73, %fd195;
	setp.lt.s64 	%p208, %rd73, 9007199254740992;
	and.pred  	%p209, %p207, %p208;
	@%p209 bra 	$L__BB0_191;

$L__BB0_190:
	mul.f64 	%fd1399, %fd194, 0d3CF0000000000000;
	setp.lt.f64 	%p210, %fd193, %fd1399;
	mul.f64 	%fd1400, %fd195, 0d3CF0000000000000;
	setp.lt.f64 	%p211, %fd193, %fd1400;
	and.pred  	%p212, %p210, %p211;
	@%p212 bra 	$L__BB0_192;

$L__BB0_191:
	sub.f64 	%fd1498, %fd1497, %fd1479;

$L__BB0_192:
	setp.gt.f64 	%p213, %fd99, 0d0000000000000000;
	setp.lt.f64 	%p214, %fd1498, 0d0000000000000000;
	and.pred  	%p215, %p213, %p214;
	@%p215 bra 	$L__BB0_194;

	setp.geu.f64 	%p216, %fd99, 0d0000000000000000;
	setp.leu.f64 	%p217, %fd1498, 0d0000000000000000;
	or.pred  	%p218, %p216, %p217;
	@%p218 bra 	$L__BB0_203;

$L__BB0_194:
	neg.f64 	%fd198, %fd99;
	setp.eq.f64 	%p219, %fd1498, %fd198;
	mov.f64 	%fd1499, 0d0000000000000000;
	@%p219 bra 	$L__BB0_204;

	setp.eq.f64 	%p220, %fd1498, 0d0000000000000000;
	setp.eq.f64 	%p221, %fd99, 0d8000000000000000;
	or.pred  	%p222, %p221, %p220;
	@%p222 bra 	$L__BB0_203;

	add.f64 	%fd1402, %fd99, %fd1498;
	abs.f64 	%fd199, %fd1402;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r291}, %fd199;
	}
	and.b32  	%r292, %r291, 2146435072;
	setp.eq.s32 	%p223, %r292, 2146435072;
	@%p223 bra 	$L__BB0_203;

	abs.f64 	%fd200, %fd1498;
	mul.f64 	%fd1403, %fd200, 0d3D30000000000000;
	setp.gt.f64 	%p224, %fd199, %fd1403;
	@%p224 bra 	$L__BB0_203;

	abs.f64 	%fd201, %fd198;
	mul.f64 	%fd1404, %fd201, 0d3D30000000000000;
	setp.gt.f64 	%p225, %fd199, %fd1404;
	@%p225 bra 	$L__BB0_203;

	setp.gtu.f64 	%p226, %fd199, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd74, %fd199;
	setp.gt.s64 	%p227, %rd74, 9007199254740991;
	or.pred  	%p228, %p226, %p227;
	@%p228 bra 	$L__BB0_202;

	setp.gtu.f64 	%p229, %fd200, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd75, %fd200;
	setp.gt.s64 	%p230, %rd75, 9007199254740991;
	or.pred  	%p231, %p229, %p230;
	@%p231 bra 	$L__BB0_202;

	setp.le.f64 	%p232, %fd201, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd76, %fd201;
	setp.lt.s64 	%p233, %rd76, 9007199254740992;
	and.pred  	%p234, %p232, %p233;
	@%p234 bra 	$L__BB0_203;

$L__BB0_202:
	mul.f64 	%fd1406, %fd200, 0d3CF0000000000000;
	setp.lt.f64 	%p235, %fd199, %fd1406;
	mul.f64 	%fd1407, %fd201, 0d3CF0000000000000;
	setp.lt.f64 	%p236, %fd199, %fd1407;
	and.pred  	%p237, %p235, %p236;
	@%p237 bra 	$L__BB0_204;

$L__BB0_203:
	add.f64 	%fd1499, %fd99, %fd1498;

$L__BB0_204:
	setp.gt.f64 	%p238, %fd66, 0d0000000000000000;
	setp.lt.f64 	%p239, %fd1499, 0d0000000000000000;
	and.pred  	%p240, %p238, %p239;
	@%p240 bra 	$L__BB0_206;

	setp.geu.f64 	%p241, %fd66, 0d0000000000000000;
	setp.leu.f64 	%p242, %fd1499, 0d0000000000000000;
	or.pred  	%p243, %p241, %p242;
	@%p243 bra 	$L__BB0_215;

$L__BB0_206:
	neg.f64 	%fd204, %fd66;
	setp.eq.f64 	%p244, %fd1499, %fd204;
	mov.f64 	%fd1500, 0d0000000000000000;
	@%p244 bra 	$L__BB0_216;

	setp.eq.f64 	%p245, %fd1499, 0d0000000000000000;
	setp.eq.f64 	%p246, %fd66, 0d8000000000000000;
	or.pred  	%p247, %p246, %p245;
	@%p247 bra 	$L__BB0_215;

	add.f64 	%fd1409, %fd66, %fd1499;
	abs.f64 	%fd205, %fd1409;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r293}, %fd205;
	}
	and.b32  	%r294, %r293, 2146435072;
	setp.eq.s32 	%p248, %r294, 2146435072;
	@%p248 bra 	$L__BB0_215;

	abs.f64 	%fd206, %fd1499;
	mul.f64 	%fd1410, %fd206, 0d3D30000000000000;
	setp.gt.f64 	%p249, %fd205, %fd1410;
	@%p249 bra 	$L__BB0_215;

	abs.f64 	%fd207, %fd204;
	mul.f64 	%fd1411, %fd207, 0d3D30000000000000;
	setp.gt.f64 	%p250, %fd205, %fd1411;
	@%p250 bra 	$L__BB0_215;

	setp.gtu.f64 	%p251, %fd205, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd77, %fd205;
	setp.gt.s64 	%p252, %rd77, 9007199254740991;
	or.pred  	%p253, %p251, %p252;
	@%p253 bra 	$L__BB0_214;

	setp.gtu.f64 	%p254, %fd206, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd78, %fd206;
	setp.gt.s64 	%p255, %rd78, 9007199254740991;
	or.pred  	%p256, %p254, %p255;
	@%p256 bra 	$L__BB0_214;

	setp.le.f64 	%p257, %fd207, 0d001FFFFFFFFFFFFF;
	mov.b64 	%rd79, %fd207;
	setp.lt.s64 	%p258, %rd79, 9007199254740992;
	and.pred  	%p259, %p257, %p258;
	@%p259 bra 	$L__BB0_215;

$L__BB0_214:
	mul.f64 	%fd1413, %fd206, 0d3CF0000000000000;
	setp.lt.f64 	%p260, %fd205, %fd1413;
	mul.f64 	%fd1414, %fd207, 0d3CF0000000000000;
	setp.lt.f64 	%p261, %fd205, %fd1414;
	and.pred  	%p262, %p260, %p261;
	@%p262 bra 	$L__BB0_216;

$L__BB0_215:
	add.f64 	%fd1500, %fd66, %fd1499;

$L__BB0_216:
	ld.param.u64 	%rd84, [DynamicKernel_nop_fsum_fsum_fsub_fsum_Var_OpNormsdist_Covar_OpPearson_Slope_param_0];
	add.s64 	%rd81, %rd84, %rd62;
	st.global.f64 	[%rd81], %fd1500;
	ret;

$L__BB0_111:
	mov.f64 	%fd993, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r219, %temp}, %fd993;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r220}, %fd993;
	}
	and.b32  	%r221, %r220, 2147483647;
	setp.ne.s32 	%p109, %r221, 2146435072;
	setp.ne.s32 	%p110, %r219, 0;
	or.pred  	%p111, %p110, %p109;
	@%p111 bra 	$L__BB0_114;
	bra.uni 	$L__BB0_112;

$L__BB0_114:
	mov.f64 	%fd998, 0d3FE0000000000000;
	mul.rn.f64 	%fd999, %fd998, %fd993;
	cvt.rzi.f64.f64 	%fd1000, %fd999;
	mov.f64 	%fd1001, 0d4000000000000000;
	mul.rn.f64 	%fd1002, %fd1001, %fd1000;
	sub.f64 	%fd1003, %fd993, %fd1002;
	abs.f64 	%fd141, %fd1003;
	setp.eq.f64 	%p114, %fd1484, 0d0000000000000000;
	@%p114 bra 	$L__BB0_131;
	bra.uni 	$L__BB0_115;

$L__BB0_131:
	setp.eq.f64 	%p130, %fd141, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1182, %fd1484;
	mov.f64 	%fd1183, 0d0000000000000000;
	rcp.rn.f64 	%fd1184, %fd1183;
	selp.f64 	%fd1489, %fd1182, %fd1184, %p130;
	bra.uni 	$L__BB0_134;

$L__BB0_144:
	mov.f64 	%fd1194, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r254, %temp}, %fd1194;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r255}, %fd1194;
	}
	and.b32  	%r256, %r255, 2147483647;
	setp.ne.s32 	%p140, %r256, 2146435072;
	setp.ne.s32 	%p141, %r254, 0;
	or.pred  	%p142, %p141, %p140;
	@%p142 bra 	$L__BB0_147;
	bra.uni 	$L__BB0_145;

$L__BB0_147:
	mov.f64 	%fd1199, 0d3FE0000000000000;
	mul.rn.f64 	%fd1200, %fd1199, %fd1194;
	cvt.rzi.f64.f64 	%fd1201, %fd1200;
	mov.f64 	%fd1202, 0d4000000000000000;
	mul.rn.f64 	%fd1203, %fd1202, %fd1201;
	sub.f64 	%fd1204, %fd1194, %fd1203;
	abs.f64 	%fd168, %fd1204;
	setp.eq.f64 	%p145, %fd165, 0d0000000000000000;
	@%p145 bra 	$L__BB0_164;
	bra.uni 	$L__BB0_148;

$L__BB0_164:
	setp.eq.f64 	%p161, %fd168, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1383, %fd165;
	mov.f64 	%fd1384, 0d0000000000000000;
	rcp.rn.f64 	%fd1385, %fd1384;
	selp.f64 	%fd1495, %fd1383, %fd1385, %p161;
	bra.uni 	$L__BB0_167;

$L__BB0_112:
	setp.eq.f64 	%p112, %fd1484, 0dBFF0000000000000;
	@%p112 bra 	$L__BB0_134;

	setp.gt.f64 	%p113, %fd139, 0d3FF0000000000000;
	mov.f64 	%fd995, 0d0000000000000000;
	rcp.rn.f64 	%fd996, %fd995;
	selp.f64 	%fd1489, 0d0000000000000000, %fd996, %p113;
	bra.uni 	$L__BB0_134;

$L__BB0_11:
	setp.eq.f64 	%p14, %fd1429, 0dBFF0000000000000;
	@%p14 bra 	$L__BB0_33;

	setp.gt.f64 	%p15, %fd15, 0d3FF0000000000000;
	mov.f64 	%fd222, 0d0000000000000000;
	rcp.rn.f64 	%fd223, %fd222;
	selp.f64 	%fd1438, 0d0000000000000000, %fd223, %p15;
	bra.uni 	$L__BB0_33;

$L__BB0_145:
	setp.eq.f64 	%p143, %fd165, 0dBFF0000000000000;
	@%p143 bra 	$L__BB0_167;

	setp.gt.f64 	%p144, %fd166, 0d3FF0000000000000;
	mov.f64 	%fd1196, 0d0000000000000000;
	rcp.rn.f64 	%fd1197, %fd1196;
	selp.f64 	%fd1495, 0d0000000000000000, %fd1197, %p144;
	bra.uni 	$L__BB0_167;

$L__BB0_43:
	mov.f64 	%fd423, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r141, %temp}, %fd423;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r142}, %fd423;
	}
	and.b32  	%r143, %r142, 2147483647;
	setp.ne.s32 	%p44, %r143, 2146435072;
	setp.ne.s32 	%p45, %r141, 0;
	or.pred  	%p46, %p45, %p44;
	@%p46 bra 	$L__BB0_46;
	bra.uni 	$L__BB0_44;

$L__BB0_46:
	mov.f64 	%fd428, 0d3FE0000000000000;
	mul.rn.f64 	%fd429, %fd428, %fd423;
	cvt.rzi.f64.f64 	%fd430, %fd429;
	mov.f64 	%fd431, 0d4000000000000000;
	mul.rn.f64 	%fd432, %fd431, %fd430;
	sub.f64 	%fd433, %fd423, %fd432;
	abs.f64 	%fd48, %fd433;
	setp.eq.f64 	%p49, %fd1441, 0dFFF0000000000000;
	@%p49 bra 	$L__BB0_60;
	bra.uni 	$L__BB0_47;

$L__BB0_60:
	setp.neu.f64 	%p63, %fd48, 0d3FF0000000000000;
	mov.f64 	%fd1448, 0d0000000000000000;
	@%p63 bra 	$L__BB0_64;

	mov.f64 	%fd1448, 0d8000000000000000;
	bra.uni 	$L__BB0_64;

$L__BB0_115:
	setp.eq.f64 	%p115, %fd1484, 0dFFF0000000000000;
	@%p115 bra 	$L__BB0_129;
	bra.uni 	$L__BB0_116;

$L__BB0_129:
	setp.neu.f64 	%p129, %fd141, 0d3FF0000000000000;
	mov.f64 	%fd1489, 0d0000000000000000;
	@%p129 bra 	$L__BB0_134;

	mov.f64 	%fd1489, 0d8000000000000000;
	bra.uni 	$L__BB0_134;

$L__BB0_14:
	setp.eq.f64 	%p17, %fd1429, 0dFFF0000000000000;
	@%p17 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_15;

$L__BB0_28:
	setp.neu.f64 	%p31, %fd17, 0d3FF0000000000000;
	mov.f64 	%fd1438, 0d0000000000000000;
	@%p31 bra 	$L__BB0_33;

	mov.f64 	%fd1438, 0d8000000000000000;
	bra.uni 	$L__BB0_33;

$L__BB0_148:
	setp.eq.f64 	%p146, %fd165, 0dFFF0000000000000;
	@%p146 bra 	$L__BB0_162;
	bra.uni 	$L__BB0_149;

$L__BB0_162:
	setp.neu.f64 	%p160, %fd168, 0d3FF0000000000000;
	mov.f64 	%fd1495, 0d0000000000000000;
	@%p160 bra 	$L__BB0_167;

	mov.f64 	%fd1495, 0d8000000000000000;
	bra.uni 	$L__BB0_167;

$L__BB0_44:
	setp.eq.f64 	%p47, %fd1441, 0dBFF0000000000000;
	@%p47 bra 	$L__BB0_64;

	setp.gt.f64 	%p48, %fd46, 0d3FF0000000000000;
	mov.f64 	%fd425, 0d0000000000000000;
	rcp.rn.f64 	%fd426, %fd425;
	selp.f64 	%fd1448, 0d0000000000000000, %fd426, %p48;
	bra.uni 	$L__BB0_64;

$L__BB0_116:
	setp.geu.f64 	%p116, %fd1484, 0d0000000000000000;
	@%p116 bra 	$L__BB0_118;

	mov.f64 	%fd1005, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd1006, %fd1005;
	setp.neu.f64 	%p117, %fd1006, 0dBFF0000000000000;
	mov.f64 	%fd1489, 0dFFF8000000000000;
	@%p117 bra 	$L__BB0_134;

$L__BB0_118:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r321}, %fd139; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r320, hi}, %fd139; 
	}
	// end inline asm
	shr.u32 	%r224, %r321, 20;
	and.b32  	%r322, %r224, 2047;
	setp.ne.s32 	%p118, %r322, 0;
	@%p118 bra 	$L__BB0_120;

	mov.f64 	%fd1011, 0d4350000000000000;
	mul.rn.f64 	%fd1010, %fd139, %fd1011;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r321}, %fd1010; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r320, hi}, %fd1010; 
	}
	// end inline asm
	shr.u32 	%r227, %r321, 20;
	and.b32  	%r228, %r227, 2047;
	add.s32 	%r322, %r228, -54;

$L__BB0_120:
	add.s32 	%r323, %r322, -1023;
	and.b32  	%r231, %r321, -2146435073;
	or.b32  	%r230, %r231, 1072693248;
	// begin inline asm
	mov.b64 	%fd1486, {%r320, %r230};
	// end inline asm
	setp.lt.u32 	%p119, %r230, 1073127583;
	@%p119 bra 	$L__BB0_122;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r232, hi}, %fd1486; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r233}, %fd1486; 
	}
	// end inline asm
	add.s32 	%r235, %r233, -1048576;
	// begin inline asm
	mov.b64 	%fd1486, {%r232, %r235};
	// end inline asm
	add.s32 	%r323, %r322, -1022;

$L__BB0_122:
	add.f64 	%fd1100, %fd1486, 0d3FF0000000000000;
	mov.f64 	%fd1101, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1102, %fd1100;
	add.f64 	%fd1042, %fd1486, 0dBFF0000000000000;
	mov.f64 	%fd1098, 0dBFF0000000000000;
	mul.rn.f64 	%fd1103, %fd1042, %fd1102;
	add.f64 	%fd1090, %fd1103, %fd1103;
	mul.rn.f64 	%fd1038, %fd1090, %fd1090;
	mov.f64 	%fd1017, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1019, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1016, %fd1017, %fd1038, %fd1019;
	// end inline asm
	mov.f64 	%fd1023, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1020, %fd1016, %fd1038, %fd1023;
	// end inline asm
	mov.f64 	%fd1027, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1024, %fd1020, %fd1038, %fd1027;
	// end inline asm
	mov.f64 	%fd1031, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1028, %fd1024, %fd1038, %fd1031;
	// end inline asm
	mov.f64 	%fd1035, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1032, %fd1028, %fd1038, %fd1035;
	// end inline asm
	mov.f64 	%fd1039, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1036, %fd1032, %fd1038, %fd1039;
	// end inline asm
	mul.rn.f64 	%fd1104, %fd1036, %fd1038;
	sub.f64 	%fd1105, %fd1042, %fd1090;
	mov.f64 	%fd1106, 0d4000000000000000;
	mul.rn.f64 	%fd1043, %fd1106, %fd1105;
	neg.f64 	%fd1041, %fd1090;
	// begin inline asm
	fma.rn.f64 	%fd1040, %fd1041, %fd1042, %fd1043;
	// end inline asm
	mul.rn.f64 	%fd1086, %fd1102, %fd1040;
	add.f64 	%fd1107, %fd1104, 0d3FB5555555555555;
	mov.f64 	%fd1108, 0d3FB5555555555555;
	sub.f64 	%fd1109, %fd1108, %fd1107;
	add.f64 	%fd1110, %fd1104, %fd1109;
	add.f64 	%fd1111, %fd1110, 0d0000000000000000;
	add.f64 	%fd1112, %fd1111, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1053, %fd1107, %fd1112;
	sub.f64 	%fd1113, %fd1107, %fd1053;
	add.f64 	%fd1057, %fd1112, %fd1113;
	mul.rn.f64 	%fd1114, %fd1053, %fd1090;
	neg.f64 	%fd1047, %fd1114;
	// begin inline asm
	fma.rn.f64 	%fd1044, %fd1053, %fd1090, %fd1047;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1048, %fd1057, %fd1086, %fd1044;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1052, %fd1053, %fd1086, %fd1048;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1056, %fd1057, %fd1090, %fd1052;
	// end inline asm
	add.f64 	%fd1069, %fd1114, %fd1056;
	sub.f64 	%fd1115, %fd1114, %fd1069;
	add.f64 	%fd1073, %fd1056, %fd1115;
	mul.rn.f64 	%fd1116, %fd1069, %fd1090;
	neg.f64 	%fd1063, %fd1116;
	// begin inline asm
	fma.rn.f64 	%fd1060, %fd1069, %fd1090, %fd1063;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1064, %fd1073, %fd1086, %fd1060;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1068, %fd1069, %fd1086, %fd1064;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1072, %fd1073, %fd1090, %fd1068;
	// end inline asm
	add.f64 	%fd1085, %fd1116, %fd1072;
	sub.f64 	%fd1117, %fd1116, %fd1085;
	add.f64 	%fd1089, %fd1072, %fd1117;
	mul.rn.f64 	%fd1118, %fd1085, %fd1090;
	neg.f64 	%fd1079, %fd1118;
	// begin inline asm
	fma.rn.f64 	%fd1076, %fd1085, %fd1090, %fd1079;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1080, %fd1089, %fd1086, %fd1076;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1084, %fd1085, %fd1086, %fd1080;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1088, %fd1089, %fd1090, %fd1084;
	// end inline asm
	add.f64 	%fd1119, %fd1118, %fd1088;
	sub.f64 	%fd1120, %fd1118, %fd1119;
	add.f64 	%fd1121, %fd1088, %fd1120;
	add.f64 	%fd1122, %fd1090, %fd1119;
	sub.f64 	%fd1123, %fd1090, %fd1122;
	add.f64 	%fd1124, %fd1119, %fd1123;
	add.f64 	%fd1125, %fd1121, %fd1124;
	add.f64 	%fd1126, %fd1086, %fd1125;
	add.f64 	%fd1127, %fd1122, %fd1126;
	sub.f64 	%fd1128, %fd1122, %fd1127;
	add.f64 	%fd1129, %fd1126, %fd1128;
	cvt.rn.f64.s32 	%fd1130, %r323;
	mov.f64 	%fd1131, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1132, %fd1130, %fd1131;
	mov.f64 	%fd1133, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1134, %fd1130, %fd1133;
	add.f64 	%fd1135, %fd1132, %fd1127;
	sub.f64 	%fd1136, %fd1132, %fd1135;
	add.f64 	%fd1137, %fd1127, %fd1136;
	add.f64 	%fd1138, %fd1129, %fd1137;
	add.f64 	%fd1139, %fd1134, %fd1138;
	add.f64 	%fd1093, %fd1135, %fd1139;
	sub.f64 	%fd1140, %fd1135, %fd1093;
	add.f64 	%fd1097, %fd1139, %fd1140;
	mul.rn.f64 	%fd1141, %fd1093, %fd1098;
	neg.f64 	%fd1095, %fd1141;
	// begin inline asm
	fma.rn.f64 	%fd1092, %fd1093, %fd1098, %fd1095;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1096, %fd1097, %fd1098, %fd1092;
	// end inline asm
	add.f64 	%fd145, %fd1141, %fd1096;
	sub.f64 	%fd1142, %fd1141, %fd145;
	add.f64 	%fd146, %fd1096, %fd1142;
	mov.f64 	%fd1143, 0d4338000000000000;
	mov.f64 	%fd1144, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1145, %fd145, %fd1144, %fd1143;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r79, %temp}, %fd1145;
	}
	mov.f64 	%fd1146, 0dC338000000000000;
	add.rn.f64 	%fd1147, %fd1145, %fd1146;
	mov.f64 	%fd1148, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1149, %fd1147, %fd1148, %fd145;
	mov.f64 	%fd1150, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1151, %fd1147, %fd1150, %fd1149;
	mov.f64 	%fd1152, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1153, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1154, %fd1153, %fd1151, %fd1152;
	mov.f64 	%fd1155, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1156, %fd1154, %fd1151, %fd1155;
	mov.f64 	%fd1157, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1158, %fd1156, %fd1151, %fd1157;
	mov.f64 	%fd1159, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1160, %fd1158, %fd1151, %fd1159;
	mov.f64 	%fd1161, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1162, %fd1160, %fd1151, %fd1161;
	mov.f64 	%fd1163, 0d3F81111111122322;
	fma.rn.f64 	%fd1164, %fd1162, %fd1151, %fd1163;
	mov.f64 	%fd1165, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1166, %fd1164, %fd1151, %fd1165;
	mov.f64 	%fd1167, 0d3FC5555555555511;
	fma.rn.f64 	%fd1168, %fd1166, %fd1151, %fd1167;
	mov.f64 	%fd1169, 0d3FE000000000000B;
	fma.rn.f64 	%fd1170, %fd1168, %fd1151, %fd1169;
	fma.rn.f64 	%fd1171, %fd1170, %fd1151, %fd1101;
	fma.rn.f64 	%fd1172, %fd1171, %fd1151, %fd1101;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r80, %temp}, %fd1172;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r81}, %fd1172;
	}
	shl.b32 	%r236, %r79, 20;
	add.s32 	%r237, %r81, %r236;
	mov.b64 	%fd1489, {%r80, %r237};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r238}, %fd145;
	}
	mov.b32 	%f11, %r238;
	abs.f32 	%f3, %f11;
	setp.lt.f32 	%p120, %f3, 0f4086232B;
	@%p120 bra 	$L__BB0_125;

	setp.lt.f64 	%p121, %fd145, 0d0000000000000000;
	add.f64 	%fd1173, %fd145, 0d7FF0000000000000;
	selp.f64 	%fd1489, 0d0000000000000000, %fd1173, %p121;
	setp.geu.f32 	%p122, %f3, 0f40874800;
	@%p122 bra 	$L__BB0_125;

	shr.u32 	%r239, %r79, 31;
	add.s32 	%r240, %r79, %r239;
	shr.s32 	%r241, %r240, 1;
	shl.b32 	%r242, %r241, 20;
	add.s32 	%r243, %r81, %r242;
	mov.b64 	%fd1174, {%r80, %r243};
	sub.s32 	%r244, %r79, %r241;
	shl.b32 	%r245, %r244, 20;
	add.s32 	%r246, %r245, 1072693248;
	mov.u32 	%r247, 0;
	mov.b64 	%fd1175, {%r247, %r246};
	mul.f64 	%fd1489, %fd1174, %fd1175;

$L__BB0_125:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r248}, %fd1489;
	}
	and.b32  	%r249, %r248, 2147483647;
	setp.eq.s32 	%p123, %r249, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r250, %temp}, %fd1489;
	}
	setp.eq.s32 	%p124, %r250, 0;
	and.pred  	%p125, %p124, %p123;
	@%p125 bra 	$L__BB0_127;

	// begin inline asm
	fma.rn.f64 	%fd1489, %fd1489, %fd146, %fd1489;
	// end inline asm

$L__BB0_127:
	setp.neu.f64 	%p126, %fd141, 0d3FF0000000000000;
	or.pred  	%p128, %p116, %p126;
	@%p128 bra 	$L__BB0_134;

	mov.b64 	%rd64, %fd1489;
	xor.b64  	%rd65, %rd64, -9223372036854775808;
	mov.b64 	%fd1489, %rd65;
	bra.uni 	$L__BB0_134;

$L__BB0_47:
	setp.geu.f64 	%p50, %fd1441, 0d0000000000000000;
	@%p50 bra 	$L__BB0_49;

	mov.f64 	%fd435, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd436, %fd435;
	setp.neu.f64 	%p51, %fd436, 0dBFF0000000000000;
	mov.f64 	%fd1448, 0dFFF8000000000000;
	@%p51 bra 	$L__BB0_64;

$L__BB0_49:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r306}, %fd46; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r305, hi}, %fd46; 
	}
	// end inline asm
	shr.u32 	%r146, %r306, 20;
	and.b32  	%r307, %r146, 2047;
	setp.ne.s32 	%p52, %r307, 0;
	@%p52 bra 	$L__BB0_51;

	mov.f64 	%fd441, 0d4350000000000000;
	mul.rn.f64 	%fd440, %fd46, %fd441;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r306}, %fd440; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r305, hi}, %fd440; 
	}
	// end inline asm
	shr.u32 	%r149, %r306, 20;
	and.b32  	%r150, %r149, 2047;
	add.s32 	%r307, %r150, -54;

$L__BB0_51:
	and.b32  	%r153, %r306, -2146435073;
	or.b32  	%r152, %r153, 1072693248;
	// begin inline asm
	mov.b64 	%fd1445, {%r305, %r152};
	// end inline asm
	setp.lt.u32 	%p53, %r152, 1073127583;
	add.s32 	%r308, %r307, -1023;
	@%p53 bra 	$L__BB0_53;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r154, hi}, %fd1445; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r155}, %fd1445; 
	}
	// end inline asm
	add.s32 	%r157, %r155, -1048576;
	// begin inline asm
	mov.b64 	%fd1445, {%r154, %r157};
	// end inline asm
	add.s32 	%r308, %r307, -1022;

$L__BB0_53:
	add.f64 	%fd530, %fd1445, 0d3FF0000000000000;
	mov.f64 	%fd531, 0d3FF0000000000000;
	rcp.rn.f64 	%fd532, %fd530;
	add.f64 	%fd472, %fd1445, 0dBFF0000000000000;
	mov.f64 	%fd528, 0dBFF0000000000000;
	mul.rn.f64 	%fd533, %fd472, %fd532;
	add.f64 	%fd520, %fd533, %fd533;
	mul.rn.f64 	%fd468, %fd520, %fd520;
	mov.f64 	%fd447, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd449, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd446, %fd447, %fd468, %fd449;
	// end inline asm
	mov.f64 	%fd453, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd450, %fd446, %fd468, %fd453;
	// end inline asm
	mov.f64 	%fd457, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd454, %fd450, %fd468, %fd457;
	// end inline asm
	mov.f64 	%fd461, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd458, %fd454, %fd468, %fd461;
	// end inline asm
	mov.f64 	%fd465, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd462, %fd458, %fd468, %fd465;
	// end inline asm
	mov.f64 	%fd469, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd466, %fd462, %fd468, %fd469;
	// end inline asm
	mul.rn.f64 	%fd534, %fd466, %fd468;
	sub.f64 	%fd535, %fd472, %fd520;
	mov.f64 	%fd536, 0d4000000000000000;
	mul.rn.f64 	%fd473, %fd536, %fd535;
	neg.f64 	%fd471, %fd520;
	// begin inline asm
	fma.rn.f64 	%fd470, %fd471, %fd472, %fd473;
	// end inline asm
	mul.rn.f64 	%fd516, %fd532, %fd470;
	add.f64 	%fd537, %fd534, 0d3FB5555555555555;
	mov.f64 	%fd538, 0d3FB5555555555555;
	sub.f64 	%fd539, %fd538, %fd537;
	add.f64 	%fd540, %fd534, %fd539;
	add.f64 	%fd541, %fd540, 0d0000000000000000;
	add.f64 	%fd542, %fd541, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd483, %fd537, %fd542;
	sub.f64 	%fd543, %fd537, %fd483;
	add.f64 	%fd487, %fd542, %fd543;
	mul.rn.f64 	%fd544, %fd483, %fd520;
	neg.f64 	%fd477, %fd544;
	// begin inline asm
	fma.rn.f64 	%fd474, %fd483, %fd520, %fd477;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd478, %fd487, %fd516, %fd474;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd482, %fd483, %fd516, %fd478;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd486, %fd487, %fd520, %fd482;
	// end inline asm
	add.f64 	%fd499, %fd544, %fd486;
	sub.f64 	%fd545, %fd544, %fd499;
	add.f64 	%fd503, %fd486, %fd545;
	mul.rn.f64 	%fd546, %fd499, %fd520;
	neg.f64 	%fd493, %fd546;
	// begin inline asm
	fma.rn.f64 	%fd490, %fd499, %fd520, %fd493;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd494, %fd503, %fd516, %fd490;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd498, %fd499, %fd516, %fd494;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd502, %fd503, %fd520, %fd498;
	// end inline asm
	add.f64 	%fd515, %fd546, %fd502;
	sub.f64 	%fd547, %fd546, %fd515;
	add.f64 	%fd519, %fd502, %fd547;
	mul.rn.f64 	%fd548, %fd515, %fd520;
	neg.f64 	%fd509, %fd548;
	// begin inline asm
	fma.rn.f64 	%fd506, %fd515, %fd520, %fd509;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd510, %fd519, %fd516, %fd506;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd514, %fd515, %fd516, %fd510;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd518, %fd519, %fd520, %fd514;
	// end inline asm
	add.f64 	%fd549, %fd548, %fd518;
	sub.f64 	%fd550, %fd548, %fd549;
	add.f64 	%fd551, %fd518, %fd550;
	add.f64 	%fd552, %fd520, %fd549;
	sub.f64 	%fd553, %fd520, %fd552;
	add.f64 	%fd554, %fd549, %fd553;
	add.f64 	%fd555, %fd551, %fd554;
	add.f64 	%fd556, %fd516, %fd555;
	add.f64 	%fd557, %fd552, %fd556;
	sub.f64 	%fd558, %fd552, %fd557;
	add.f64 	%fd559, %fd556, %fd558;
	cvt.rn.f64.s32 	%fd560, %r308;
	mov.f64 	%fd561, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd562, %fd560, %fd561;
	mov.f64 	%fd563, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd564, %fd560, %fd563;
	add.f64 	%fd565, %fd562, %fd557;
	sub.f64 	%fd566, %fd562, %fd565;
	add.f64 	%fd567, %fd557, %fd566;
	add.f64 	%fd568, %fd559, %fd567;
	add.f64 	%fd569, %fd564, %fd568;
	add.f64 	%fd523, %fd565, %fd569;
	sub.f64 	%fd570, %fd565, %fd523;
	add.f64 	%fd527, %fd569, %fd570;
	mul.rn.f64 	%fd571, %fd523, %fd528;
	neg.f64 	%fd525, %fd571;
	// begin inline asm
	fma.rn.f64 	%fd522, %fd523, %fd528, %fd525;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd526, %fd527, %fd528, %fd522;
	// end inline asm
	add.f64 	%fd52, %fd571, %fd526;
	sub.f64 	%fd572, %fd571, %fd52;
	add.f64 	%fd53, %fd526, %fd572;
	mov.f64 	%fd573, 0d4338000000000000;
	mov.f64 	%fd574, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd575, %fd52, %fd574, %fd573;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r38, %temp}, %fd575;
	}
	mov.f64 	%fd576, 0dC338000000000000;
	add.rn.f64 	%fd577, %fd575, %fd576;
	mov.f64 	%fd578, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd579, %fd577, %fd578, %fd52;
	mov.f64 	%fd580, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd581, %fd577, %fd580, %fd579;
	mov.f64 	%fd582, 0d3E928AF3FCA213EA;
	mov.f64 	%fd583, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd584, %fd583, %fd581, %fd582;
	mov.f64 	%fd585, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd586, %fd584, %fd581, %fd585;
	mov.f64 	%fd587, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd588, %fd586, %fd581, %fd587;
	mov.f64 	%fd589, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd590, %fd588, %fd581, %fd589;
	mov.f64 	%fd591, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd592, %fd590, %fd581, %fd591;
	mov.f64 	%fd593, 0d3F81111111122322;
	fma.rn.f64 	%fd594, %fd592, %fd581, %fd593;
	mov.f64 	%fd595, 0d3FA55555555502A1;
	fma.rn.f64 	%fd596, %fd594, %fd581, %fd595;
	mov.f64 	%fd597, 0d3FC5555555555511;
	fma.rn.f64 	%fd598, %fd596, %fd581, %fd597;
	mov.f64 	%fd599, 0d3FE000000000000B;
	fma.rn.f64 	%fd600, %fd598, %fd581, %fd599;
	fma.rn.f64 	%fd601, %fd600, %fd581, %fd531;
	fma.rn.f64 	%fd602, %fd601, %fd581, %fd531;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r39, %temp}, %fd602;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r40}, %fd602;
	}
	shl.b32 	%r158, %r38, 20;
	add.s32 	%r159, %r40, %r158;
	mov.b64 	%fd1448, {%r39, %r159};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r160}, %fd52;
	}
	mov.b32 	%f6, %r160;
	abs.f32 	%f2, %f6;
	setp.lt.f32 	%p54, %f2, 0f4086232B;
	@%p54 bra 	$L__BB0_56;

	setp.lt.f64 	%p55, %fd52, 0d0000000000000000;
	add.f64 	%fd603, %fd52, 0d7FF0000000000000;
	selp.f64 	%fd1448, 0d0000000000000000, %fd603, %p55;
	setp.geu.f32 	%p56, %f2, 0f40874800;
	@%p56 bra 	$L__BB0_56;

	mov.f64 	%fd1425, 0d4338000000000000;
	mov.f64 	%fd1424, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1423, %fd52, %fd1424, %fd1425;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r296, %temp}, %fd1423;
	}
	shr.u32 	%r161, %r296, 31;
	add.s32 	%r162, %r296, %r161;
	shr.s32 	%r163, %r162, 1;
	shl.b32 	%r164, %r163, 20;
	add.s32 	%r165, %r40, %r164;
	mov.b64 	%fd604, {%r39, %r165};
	sub.s32 	%r166, %r296, %r163;
	shl.b32 	%r167, %r166, 20;
	add.s32 	%r168, %r167, 1072693248;
	mov.u32 	%r169, 0;
	mov.b64 	%fd605, {%r169, %r168};
	mul.f64 	%fd1448, %fd604, %fd605;

$L__BB0_56:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r170}, %fd1448;
	}
	and.b32  	%r171, %r170, 2147483647;
	setp.eq.s32 	%p57, %r171, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r172, %temp}, %fd1448;
	}
	setp.eq.s32 	%p58, %r172, 0;
	and.pred  	%p59, %p58, %p57;
	@%p59 bra 	$L__BB0_58;

	// begin inline asm
	fma.rn.f64 	%fd1448, %fd1448, %fd53, %fd1448;
	// end inline asm

$L__BB0_58:
	setp.neu.f64 	%p60, %fd48, 0d3FF0000000000000;
	or.pred  	%p62, %p50, %p60;
	@%p62 bra 	$L__BB0_64;

	mov.b64 	%rd54, %fd1448;
	xor.b64  	%rd55, %rd54, -9223372036854775808;
	mov.b64 	%fd1448, %rd55;
	bra.uni 	$L__BB0_64;

$L__BB0_15:
	setp.geu.f64 	%p18, %fd1429, 0d0000000000000000;
	@%p18 bra 	$L__BB0_17;

	mov.f64 	%fd232, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd233, %fd232;
	setp.neu.f64 	%p19, %fd233, 0dBFF0000000000000;
	mov.f64 	%fd1438, 0dFFF8000000000000;
	@%p19 bra 	$L__BB0_33;

$L__BB0_17:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r300}, %fd15; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r299, hi}, %fd15; 
	}
	// end inline asm
	shr.u32 	%r110, %r300, 20;
	and.b32  	%r301, %r110, 2047;
	setp.ne.s32 	%p20, %r301, 0;
	@%p20 bra 	$L__BB0_19;

	mov.f64 	%fd238, 0d4350000000000000;
	mul.rn.f64 	%fd237, %fd15, %fd238;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r300}, %fd237; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r299, hi}, %fd237; 
	}
	// end inline asm
	shr.u32 	%r113, %r300, 20;
	and.b32  	%r114, %r113, 2047;
	add.s32 	%r301, %r114, -54;

$L__BB0_19:
	and.b32  	%r117, %r300, -2146435073;
	or.b32  	%r116, %r117, 1072693248;
	// begin inline asm
	mov.b64 	%fd1435, {%r299, %r116};
	// end inline asm
	setp.lt.u32 	%p21, %r116, 1073127583;
	add.s32 	%r302, %r301, -1023;
	@%p21 bra 	$L__BB0_21;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r118, hi}, %fd1435; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r119}, %fd1435; 
	}
	// end inline asm
	add.s32 	%r121, %r119, -1048576;
	// begin inline asm
	mov.b64 	%fd1435, {%r118, %r121};
	// end inline asm
	add.s32 	%r302, %r301, -1022;

$L__BB0_21:
	add.f64 	%fd327, %fd1435, 0d3FF0000000000000;
	mov.f64 	%fd328, 0d3FF0000000000000;
	rcp.rn.f64 	%fd329, %fd327;
	add.f64 	%fd269, %fd1435, 0dBFF0000000000000;
	mov.f64 	%fd325, 0dBFF0000000000000;
	mul.rn.f64 	%fd330, %fd269, %fd329;
	add.f64 	%fd317, %fd330, %fd330;
	mul.rn.f64 	%fd265, %fd317, %fd317;
	mov.f64 	%fd244, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd246, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd243, %fd244, %fd265, %fd246;
	// end inline asm
	mov.f64 	%fd250, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd247, %fd243, %fd265, %fd250;
	// end inline asm
	mov.f64 	%fd254, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd251, %fd247, %fd265, %fd254;
	// end inline asm
	mov.f64 	%fd258, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd255, %fd251, %fd265, %fd258;
	// end inline asm
	mov.f64 	%fd262, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd259, %fd255, %fd265, %fd262;
	// end inline asm
	mov.f64 	%fd266, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd263, %fd259, %fd265, %fd266;
	// end inline asm
	mul.rn.f64 	%fd331, %fd263, %fd265;
	sub.f64 	%fd332, %fd269, %fd317;
	mov.f64 	%fd333, 0d4000000000000000;
	mul.rn.f64 	%fd270, %fd333, %fd332;
	neg.f64 	%fd268, %fd317;
	// begin inline asm
	fma.rn.f64 	%fd267, %fd268, %fd269, %fd270;
	// end inline asm
	mul.rn.f64 	%fd313, %fd329, %fd267;
	add.f64 	%fd334, %fd331, 0d3FB5555555555555;
	mov.f64 	%fd335, 0d3FB5555555555555;
	sub.f64 	%fd336, %fd335, %fd334;
	add.f64 	%fd337, %fd331, %fd336;
	add.f64 	%fd338, %fd337, 0d0000000000000000;
	add.f64 	%fd339, %fd338, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd280, %fd334, %fd339;
	sub.f64 	%fd340, %fd334, %fd280;
	add.f64 	%fd284, %fd339, %fd340;
	mul.rn.f64 	%fd341, %fd280, %fd317;
	neg.f64 	%fd274, %fd341;
	// begin inline asm
	fma.rn.f64 	%fd271, %fd280, %fd317, %fd274;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd275, %fd284, %fd313, %fd271;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd279, %fd280, %fd313, %fd275;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd283, %fd284, %fd317, %fd279;
	// end inline asm
	add.f64 	%fd296, %fd341, %fd283;
	sub.f64 	%fd342, %fd341, %fd296;
	add.f64 	%fd300, %fd283, %fd342;
	mul.rn.f64 	%fd343, %fd296, %fd317;
	neg.f64 	%fd290, %fd343;
	// begin inline asm
	fma.rn.f64 	%fd287, %fd296, %fd317, %fd290;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd291, %fd300, %fd313, %fd287;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd295, %fd296, %fd313, %fd291;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd299, %fd300, %fd317, %fd295;
	// end inline asm
	add.f64 	%fd312, %fd343, %fd299;
	sub.f64 	%fd344, %fd343, %fd312;
	add.f64 	%fd316, %fd299, %fd344;
	mul.rn.f64 	%fd345, %fd312, %fd317;
	neg.f64 	%fd306, %fd345;
	// begin inline asm
	fma.rn.f64 	%fd303, %fd312, %fd317, %fd306;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd307, %fd316, %fd313, %fd303;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd311, %fd312, %fd313, %fd307;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd315, %fd316, %fd317, %fd311;
	// end inline asm
	add.f64 	%fd346, %fd345, %fd315;
	sub.f64 	%fd347, %fd345, %fd346;
	add.f64 	%fd348, %fd315, %fd347;
	add.f64 	%fd349, %fd317, %fd346;
	sub.f64 	%fd350, %fd317, %fd349;
	add.f64 	%fd351, %fd346, %fd350;
	add.f64 	%fd352, %fd348, %fd351;
	add.f64 	%fd353, %fd313, %fd352;
	add.f64 	%fd354, %fd349, %fd353;
	sub.f64 	%fd355, %fd349, %fd354;
	add.f64 	%fd356, %fd353, %fd355;
	cvt.rn.f64.s32 	%fd357, %r302;
	mov.f64 	%fd358, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd359, %fd357, %fd358;
	mov.f64 	%fd360, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd361, %fd357, %fd360;
	add.f64 	%fd362, %fd359, %fd354;
	sub.f64 	%fd363, %fd359, %fd362;
	add.f64 	%fd364, %fd354, %fd363;
	add.f64 	%fd365, %fd356, %fd364;
	add.f64 	%fd366, %fd361, %fd365;
	add.f64 	%fd320, %fd362, %fd366;
	sub.f64 	%fd367, %fd362, %fd320;
	add.f64 	%fd324, %fd366, %fd367;
	mul.rn.f64 	%fd368, %fd320, %fd325;
	neg.f64 	%fd322, %fd368;
	// begin inline asm
	fma.rn.f64 	%fd319, %fd320, %fd325, %fd322;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd323, %fd324, %fd325, %fd319;
	// end inline asm
	add.f64 	%fd21, %fd368, %fd323;
	sub.f64 	%fd369, %fd368, %fd21;
	add.f64 	%fd22, %fd323, %fd369;
	mov.f64 	%fd370, 0d4338000000000000;
	mov.f64 	%fd371, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd372, %fd21, %fd371, %fd370;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r18, %temp}, %fd372;
	}
	mov.f64 	%fd373, 0dC338000000000000;
	add.rn.f64 	%fd374, %fd372, %fd373;
	mov.f64 	%fd375, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd376, %fd374, %fd375, %fd21;
	mov.f64 	%fd377, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd378, %fd374, %fd377, %fd376;
	mov.f64 	%fd379, 0d3E928AF3FCA213EA;
	mov.f64 	%fd380, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd381, %fd380, %fd378, %fd379;
	mov.f64 	%fd382, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd383, %fd381, %fd378, %fd382;
	mov.f64 	%fd384, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd385, %fd383, %fd378, %fd384;
	mov.f64 	%fd386, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd387, %fd385, %fd378, %fd386;
	mov.f64 	%fd388, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd389, %fd387, %fd378, %fd388;
	mov.f64 	%fd390, 0d3F81111111122322;
	fma.rn.f64 	%fd391, %fd389, %fd378, %fd390;
	mov.f64 	%fd392, 0d3FA55555555502A1;
	fma.rn.f64 	%fd393, %fd391, %fd378, %fd392;
	mov.f64 	%fd394, 0d3FC5555555555511;
	fma.rn.f64 	%fd395, %fd393, %fd378, %fd394;
	mov.f64 	%fd396, 0d3FE000000000000B;
	fma.rn.f64 	%fd397, %fd395, %fd378, %fd396;
	fma.rn.f64 	%fd398, %fd397, %fd378, %fd328;
	fma.rn.f64 	%fd399, %fd398, %fd378, %fd328;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd399;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd399;
	}
	shl.b32 	%r122, %r18, 20;
	add.s32 	%r123, %r20, %r122;
	mov.b64 	%fd1438, {%r19, %r123};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r124}, %fd21;
	}
	mov.b32 	%f5, %r124;
	abs.f32 	%f1, %f5;
	setp.lt.f32 	%p22, %f1, 0f4086232B;
	@%p22 bra 	$L__BB0_24;

	setp.lt.f64 	%p23, %fd21, 0d0000000000000000;
	add.f64 	%fd400, %fd21, 0d7FF0000000000000;
	selp.f64 	%fd1438, 0d0000000000000000, %fd400, %p23;
	setp.geu.f32 	%p24, %f1, 0f40874800;
	@%p24 bra 	$L__BB0_24;

	mov.f64 	%fd1422, 0d4338000000000000;
	mov.f64 	%fd1421, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1420, %fd21, %fd1421, %fd1422;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r295, %temp}, %fd1420;
	}
	shr.u32 	%r125, %r295, 31;
	add.s32 	%r126, %r295, %r125;
	shr.s32 	%r127, %r126, 1;
	shl.b32 	%r128, %r127, 20;
	add.s32 	%r129, %r20, %r128;
	mov.b64 	%fd401, {%r19, %r129};
	sub.s32 	%r130, %r295, %r127;
	shl.b32 	%r131, %r130, 20;
	add.s32 	%r132, %r131, 1072693248;
	mov.u32 	%r133, 0;
	mov.b64 	%fd402, {%r133, %r132};
	mul.f64 	%fd1438, %fd401, %fd402;

$L__BB0_24:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r134}, %fd1438;
	}
	and.b32  	%r135, %r134, 2147483647;
	setp.eq.s32 	%p25, %r135, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r136, %temp}, %fd1438;
	}
	setp.eq.s32 	%p26, %r136, 0;
	and.pred  	%p27, %p26, %p25;
	@%p27 bra 	$L__BB0_26;

	// begin inline asm
	fma.rn.f64 	%fd1438, %fd1438, %fd22, %fd1438;
	// end inline asm

$L__BB0_26:
	setp.neu.f64 	%p28, %fd17, 0d3FF0000000000000;
	or.pred  	%p30, %p18, %p28;
	@%p30 bra 	$L__BB0_33;

	mov.b64 	%rd51, %fd1438;
	xor.b64  	%rd52, %rd51, -9223372036854775808;
	mov.b64 	%fd1438, %rd52;
	bra.uni 	$L__BB0_33;

$L__BB0_149:
	setp.geu.f64 	%p147, %fd165, 0d0000000000000000;
	@%p147 bra 	$L__BB0_151;

	mov.f64 	%fd1206, 0dBFF0000000000000;
	cvt.rzi.f64.f64 	%fd1207, %fd1206;
	setp.neu.f64 	%p148, %fd1207, 0dBFF0000000000000;
	mov.f64 	%fd1495, 0dFFF8000000000000;
	@%p148 bra 	$L__BB0_167;

$L__BB0_151:
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r325}, %fd166; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r324, hi}, %fd166; 
	}
	// end inline asm
	shr.u32 	%r259, %r325, 20;
	and.b32  	%r326, %r259, 2047;
	setp.ne.s32 	%p149, %r326, 0;
	@%p149 bra 	$L__BB0_153;

	mov.f64 	%fd1212, 0d4350000000000000;
	mul.rn.f64 	%fd1211, %fd166, %fd1212;
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r325}, %fd1211; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r324, hi}, %fd1211; 
	}
	// end inline asm
	shr.u32 	%r262, %r325, 20;
	and.b32  	%r263, %r262, 2047;
	add.s32 	%r326, %r263, -54;

$L__BB0_153:
	add.s32 	%r327, %r326, -1023;
	and.b32  	%r266, %r325, -2146435073;
	or.b32  	%r265, %r266, 1072693248;
	// begin inline asm
	mov.b64 	%fd1492, {%r324, %r265};
	// end inline asm
	setp.lt.u32 	%p150, %r265, 1073127583;
	@%p150 bra 	$L__BB0_155;

	// begin inline asm
	{ 
	.reg 	.b32 hi; 
	mov.b64 	{%r267, hi}, %fd1492; 
	}
	// end inline asm
	// begin inline asm
	{ 
	.reg 	.b32 lo; 
	mov.b64 	{lo, %r268}, %fd1492; 
	}
	// end inline asm
	add.s32 	%r270, %r268, -1048576;
	// begin inline asm
	mov.b64 	%fd1492, {%r267, %r270};
	// end inline asm
	add.s32 	%r327, %r326, -1022;

$L__BB0_155:
	add.f64 	%fd1301, %fd1492, 0d3FF0000000000000;
	mov.f64 	%fd1302, 0d3FF0000000000000;
	rcp.rn.f64 	%fd1303, %fd1301;
	add.f64 	%fd1243, %fd1492, 0dBFF0000000000000;
	mov.f64 	%fd1299, 0dBFF0000000000000;
	mul.rn.f64 	%fd1304, %fd1243, %fd1303;
	add.f64 	%fd1291, %fd1304, %fd1304;
	mul.rn.f64 	%fd1239, %fd1291, %fd1291;
	mov.f64 	%fd1218, 0d3EB0F5FF7D2CAFE2;
	mov.f64 	%fd1220, 0d3ED0F5D241AD3B5A;
	// begin inline asm
	fma.rn.f64 	%fd1217, %fd1218, %fd1239, %fd1220;
	// end inline asm
	mov.f64 	%fd1224, 0d3EF3B20A75488A3F;
	// begin inline asm
	fma.rn.f64 	%fd1221, %fd1217, %fd1239, %fd1224;
	// end inline asm
	mov.f64 	%fd1228, 0d3F1745CDE4FAECD5;
	// begin inline asm
	fma.rn.f64 	%fd1225, %fd1221, %fd1239, %fd1228;
	// end inline asm
	mov.f64 	%fd1232, 0d3F3C71C7258A578B;
	// begin inline asm
	fma.rn.f64 	%fd1229, %fd1225, %fd1239, %fd1232;
	// end inline asm
	mov.f64 	%fd1236, 0d3F6249249242B910;
	// begin inline asm
	fma.rn.f64 	%fd1233, %fd1229, %fd1239, %fd1236;
	// end inline asm
	mov.f64 	%fd1240, 0d3F89999999999DFB;
	// begin inline asm
	fma.rn.f64 	%fd1237, %fd1233, %fd1239, %fd1240;
	// end inline asm
	mul.rn.f64 	%fd1305, %fd1237, %fd1239;
	sub.f64 	%fd1306, %fd1243, %fd1291;
	mov.f64 	%fd1307, 0d4000000000000000;
	mul.rn.f64 	%fd1244, %fd1307, %fd1306;
	neg.f64 	%fd1242, %fd1291;
	// begin inline asm
	fma.rn.f64 	%fd1241, %fd1242, %fd1243, %fd1244;
	// end inline asm
	mul.rn.f64 	%fd1287, %fd1303, %fd1241;
	add.f64 	%fd1308, %fd1305, 0d3FB5555555555555;
	mov.f64 	%fd1309, 0d3FB5555555555555;
	sub.f64 	%fd1310, %fd1309, %fd1308;
	add.f64 	%fd1311, %fd1305, %fd1310;
	add.f64 	%fd1312, %fd1311, 0d0000000000000000;
	add.f64 	%fd1313, %fd1312, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd1254, %fd1308, %fd1313;
	sub.f64 	%fd1314, %fd1308, %fd1254;
	add.f64 	%fd1258, %fd1313, %fd1314;
	mul.rn.f64 	%fd1315, %fd1254, %fd1291;
	neg.f64 	%fd1248, %fd1315;
	// begin inline asm
	fma.rn.f64 	%fd1245, %fd1254, %fd1291, %fd1248;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1249, %fd1258, %fd1287, %fd1245;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1253, %fd1254, %fd1287, %fd1249;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1257, %fd1258, %fd1291, %fd1253;
	// end inline asm
	add.f64 	%fd1270, %fd1315, %fd1257;
	sub.f64 	%fd1316, %fd1315, %fd1270;
	add.f64 	%fd1274, %fd1257, %fd1316;
	mul.rn.f64 	%fd1317, %fd1270, %fd1291;
	neg.f64 	%fd1264, %fd1317;
	// begin inline asm
	fma.rn.f64 	%fd1261, %fd1270, %fd1291, %fd1264;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1265, %fd1274, %fd1287, %fd1261;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1269, %fd1270, %fd1287, %fd1265;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1273, %fd1274, %fd1291, %fd1269;
	// end inline asm
	add.f64 	%fd1286, %fd1317, %fd1273;
	sub.f64 	%fd1318, %fd1317, %fd1286;
	add.f64 	%fd1290, %fd1273, %fd1318;
	mul.rn.f64 	%fd1319, %fd1286, %fd1291;
	neg.f64 	%fd1280, %fd1319;
	// begin inline asm
	fma.rn.f64 	%fd1277, %fd1286, %fd1291, %fd1280;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1281, %fd1290, %fd1287, %fd1277;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1285, %fd1286, %fd1287, %fd1281;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1289, %fd1290, %fd1291, %fd1285;
	// end inline asm
	add.f64 	%fd1320, %fd1319, %fd1289;
	sub.f64 	%fd1321, %fd1319, %fd1320;
	add.f64 	%fd1322, %fd1289, %fd1321;
	add.f64 	%fd1323, %fd1291, %fd1320;
	sub.f64 	%fd1324, %fd1291, %fd1323;
	add.f64 	%fd1325, %fd1320, %fd1324;
	add.f64 	%fd1326, %fd1322, %fd1325;
	add.f64 	%fd1327, %fd1287, %fd1326;
	add.f64 	%fd1328, %fd1323, %fd1327;
	sub.f64 	%fd1329, %fd1323, %fd1328;
	add.f64 	%fd1330, %fd1327, %fd1329;
	cvt.rn.f64.s32 	%fd1331, %r327;
	mov.f64 	%fd1332, 0d3FE62E42FEFA3000;
	mul.rn.f64 	%fd1333, %fd1331, %fd1332;
	mov.f64 	%fd1334, 0d3D53DE6AF278ECE6;
	mul.rn.f64 	%fd1335, %fd1331, %fd1334;
	add.f64 	%fd1336, %fd1333, %fd1328;
	sub.f64 	%fd1337, %fd1333, %fd1336;
	add.f64 	%fd1338, %fd1328, %fd1337;
	add.f64 	%fd1339, %fd1330, %fd1338;
	add.f64 	%fd1340, %fd1335, %fd1339;
	add.f64 	%fd1294, %fd1336, %fd1340;
	sub.f64 	%fd1341, %fd1336, %fd1294;
	add.f64 	%fd1298, %fd1340, %fd1341;
	mul.rn.f64 	%fd1342, %fd1294, %fd1299;
	neg.f64 	%fd1296, %fd1342;
	// begin inline asm
	fma.rn.f64 	%fd1293, %fd1294, %fd1299, %fd1296;
	// end inline asm
	// begin inline asm
	fma.rn.f64 	%fd1297, %fd1298, %fd1299, %fd1293;
	// end inline asm
	add.f64 	%fd172, %fd1342, %fd1297;
	sub.f64 	%fd1343, %fd1342, %fd172;
	add.f64 	%fd173, %fd1297, %fd1343;
	mov.f64 	%fd1344, 0d4338000000000000;
	mov.f64 	%fd1345, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1346, %fd172, %fd1345, %fd1344;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r94, %temp}, %fd1346;
	}
	mov.f64 	%fd1347, 0dC338000000000000;
	add.rn.f64 	%fd1348, %fd1346, %fd1347;
	mov.f64 	%fd1349, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1350, %fd1348, %fd1349, %fd172;
	mov.f64 	%fd1351, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1352, %fd1348, %fd1351, %fd1350;
	mov.f64 	%fd1353, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1354, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1355, %fd1354, %fd1352, %fd1353;
	mov.f64 	%fd1356, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1357, %fd1355, %fd1352, %fd1356;
	mov.f64 	%fd1358, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1359, %fd1357, %fd1352, %fd1358;
	mov.f64 	%fd1360, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1361, %fd1359, %fd1352, %fd1360;
	mov.f64 	%fd1362, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1363, %fd1361, %fd1352, %fd1362;
	mov.f64 	%fd1364, 0d3F81111111122322;
	fma.rn.f64 	%fd1365, %fd1363, %fd1352, %fd1364;
	mov.f64 	%fd1366, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1367, %fd1365, %fd1352, %fd1366;
	mov.f64 	%fd1368, 0d3FC5555555555511;
	fma.rn.f64 	%fd1369, %fd1367, %fd1352, %fd1368;
	mov.f64 	%fd1370, 0d3FE000000000000B;
	fma.rn.f64 	%fd1371, %fd1369, %fd1352, %fd1370;
	fma.rn.f64 	%fd1372, %fd1371, %fd1352, %fd1302;
	fma.rn.f64 	%fd1373, %fd1372, %fd1352, %fd1302;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r95, %temp}, %fd1373;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r96}, %fd1373;
	}
	shl.b32 	%r271, %r94, 20;
	add.s32 	%r272, %r96, %r271;
	mov.b64 	%fd1495, {%r95, %r272};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r273}, %fd172;
	}
	mov.b32 	%f12, %r273;
	abs.f32 	%f4, %f12;
	setp.lt.f32 	%p151, %f4, 0f4086232B;
	@%p151 bra 	$L__BB0_158;

	setp.lt.f64 	%p152, %fd172, 0d0000000000000000;
	add.f64 	%fd1374, %fd172, 0d7FF0000000000000;
	selp.f64 	%fd1495, 0d0000000000000000, %fd1374, %p152;
	setp.geu.f32 	%p153, %f4, 0f40874800;
	@%p153 bra 	$L__BB0_158;

	shr.u32 	%r274, %r94, 31;
	add.s32 	%r275, %r94, %r274;
	shr.s32 	%r276, %r275, 1;
	shl.b32 	%r277, %r276, 20;
	add.s32 	%r278, %r96, %r277;
	mov.b64 	%fd1375, {%r95, %r278};
	sub.s32 	%r279, %r94, %r276;
	shl.b32 	%r280, %r279, 20;
	add.s32 	%r281, %r280, 1072693248;
	mov.u32 	%r282, 0;
	mov.b64 	%fd1376, {%r282, %r281};
	mul.f64 	%fd1495, %fd1375, %fd1376;

$L__BB0_158:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r283}, %fd1495;
	}
	and.b32  	%r284, %r283, 2147483647;
	setp.eq.s32 	%p154, %r284, 2146435072;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r285, %temp}, %fd1495;
	}
	setp.eq.s32 	%p155, %r285, 0;
	and.pred  	%p156, %p155, %p154;
	@%p156 bra 	$L__BB0_160;

	// begin inline asm
	fma.rn.f64 	%fd1495, %fd1495, %fd173, %fd1495;
	// end inline asm

$L__BB0_160:
	setp.neu.f64 	%p157, %fd168, 0d3FF0000000000000;
	or.pred  	%p159, %p147, %p157;
	@%p159 bra 	$L__BB0_167;

	mov.b64 	%rd66, %fd1495;
	xor.b64  	%rd67, %rd66, -9223372036854775808;
	mov.b64 	%fd1495, %rd67;
	bra.uni 	$L__BB0_167;

}

  